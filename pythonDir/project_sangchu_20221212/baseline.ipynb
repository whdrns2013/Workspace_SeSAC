{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73bdeabb-090e-4497-b1bb-2f52aba86361",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e3652e-b5ec-4436-8a89-ebfb15f07298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b54dca-e768-45e9-b9e1-20824dc49e84",
   "metadata": {},
   "source": [
    "# Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "990e8342-769a-47de-b124-ae87f48ba9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':100,\n",
    "    'LEARNING_RATE':1e-3,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99808031-8ce3-425d-a22f-fddc06dbe18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1101a841-29ec-4360-a99d-e3db4122b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_list = sorted(glob.glob('data/train_input/*.csv'))\n",
    "all_target_list = sorted(glob.glob('data/train_target/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55382a02-5093-4654-9129-828e439ea28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습: 25, 검증: 3\n"
     ]
    }
   ],
   "source": [
    "train_input_list = all_input_list[:25]\n",
    "train_target_list = all_target_list[:25]\n",
    "\n",
    "val_input_list = all_input_list[25:]\n",
    "val_target_list = all_target_list[25:]\n",
    "\n",
    "print(f'학습: {len(train_input_list)}, 검증: {len(val_input_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0b31c3-c983-45e9-ba03-17516d5ca6ea",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09e67e77-febe-464c-bcc1-9d816737c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader(tf.keras.utils.Sequence):\n",
    "    def __init__(self, input_paths, target_paths, batch_size, infer_mode, shuffle=False):\n",
    "        self.input_paths = input_paths\n",
    "        self.target_paths = target_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.infer_mode = infer_mode\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.data_list = []\n",
    "        self.label_list = []\n",
    "        print('Data Pre-processing..')\n",
    "        for input_path, target_path in tqdm(zip(self.input_paths, self.target_paths)):\n",
    "            input_df = pd.read_csv(input_path)\n",
    "            target_df = pd.read_csv(target_path)\n",
    "\n",
    "            input_df = input_df.drop(columns=['obs_time'])\n",
    "            input_df = input_df.fillna(0)\n",
    "\n",
    "\n",
    "            target_length = int(len(target_df))\n",
    "\n",
    "            for idx in range(target_length):\n",
    "                time_series = input_df[24*idx:24*(idx+1)].values\n",
    "                self.data_list.append(time_series)\n",
    "\n",
    "            for label in target_df['predicted_weight_g']:\n",
    "                self.label_list.append(label)\n",
    "        print('Done. \\n')\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data_list)/self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "\n",
    "        data = [self.data_list[i] for i in indices]\n",
    "        label = [self.label_list[i] for i in indices]\n",
    "\n",
    "        if self.infer_mode == False:\n",
    "            return tf.convert_to_tensor(data), tf.convert_to_tensor(label)\n",
    "        else:\n",
    "            return tf.convert_to_tensor(data)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.data_list))\n",
    "\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e10f5e8-8759-4157-a43f-b8bd428fdb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Pre-processing..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:00, 326.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. \n",
      "\n",
      "Data Pre-processing..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 323.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = Dataloader(train_input_list, train_target_list, CFG['BATCH_SIZE'], False, shuffle=True)\n",
    "val_loader = Dataloader(val_input_list, val_target_list, CFG['BATCH_SIZE'], False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a785fbea-de97-4bbd-93c4-eb656a6e61cb",
   "metadata": {},
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc7c8433-bb86-43ac-b297-b306ce23a09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.lstm = tf.keras.layers.LSTM(256)\n",
    "        self.classifier = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        h = self.lstm(inputs)\n",
    "        return self.classifier(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dc2569-5fab-414d-9e34-2f31e7d18196",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beaf979-9b4d-46a0-8198-b90d92469db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModel()\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG['LEARNING_RATE']),\n",
    "    loss=tf.keras.losses.MeanAbsoluteError()\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_loader, validation_data=val_loader, \n",
    "    epochs=CFG['EPOCHS'],\n",
    "    callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-8, verbose=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3e625-5f1b-403c-82af-399c8a528f97",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded9674-09bd-4d04-ba5f-21d8a5c4b5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_list = sorted(glob.glob('data/test_input/*.csv'))\n",
    "test_target_list = sorted(glob.glob('data/test_target/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f7128-0f5d-4ed3-bd48-f1b2790ed623",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_input_path, test_target_path in zip(test_input_list, test_target_list):\n",
    "    print(test_target_path)\n",
    "    test_loader = Dataloader([test_input_path], [test_target_path], CFG['BATCH_SIZE'], True, shuffle=False)\n",
    "    model_pred = model.predict(test_loader)\n",
    "\n",
    "    submit_df = pd.read_csv(test_target_path)\n",
    "    submit_df['predicted_weight_g'] = model_pred\n",
    "    submit_df.to_csv(test_target_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33bf8fd-82cc-492e-b144-af44c4f48f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "os.chdir(\"data/test_target/\")\n",
    "submission = zipfile.ZipFile(\"submission.zip\", 'w')\n",
    "for path in test_target_list:\n",
    "    path = path.split('/')[-1]\n",
    "    submission.write(path)\n",
    "submission.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f58cc-b28a-4fa1-bc5d-e692231d8960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
