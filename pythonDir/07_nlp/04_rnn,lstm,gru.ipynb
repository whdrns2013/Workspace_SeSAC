{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09dde943-cb01-4aea-93e2-4e9fc383082d",
   "metadata": {},
   "source": [
    "# 강사님 생각"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb9b81a-fbef-43f1-ab9a-40b1d92e99cd",
   "metadata": {},
   "source": [
    "## 좋은 회사란??  \n",
    "* 세미나를 많이 하는 회사라고 생각한다.  \n",
    "* 이는, 계속 성장하려는 회사라는 것이고  \n",
    "* 직원들이 공부할 수 있게 하는 회사라는 것이다.  \n",
    "* 규모의 문제가 아님  \n",
    "* 계속 외부 인재를 통한 인사이트 업그레이드는 불가능하다.  \n",
    "* 내부 인재들에게 성장하게 하고, 이를 세미나를 통해 발표하는 것. 아주 바람직하다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aacb0c-4c38-430c-9afe-35b9ed0df094",
   "metadata": {},
   "source": [
    "# 텍스트 딥러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72d6930-0d93-4c36-b1ed-badc078205c8",
   "metadata": {},
   "source": [
    "* 텍스트 딥러닝에서 알아둘 개념 : RNN, LSTM, GRU\n",
    "* LSTM, GRU는 RNN의 파생형\n",
    "* Seq2seq : 문장 to 문장. RNN을 사용하는 개념이다. 이를 통해 번역이나 챗봇, 요약 등이 가능하다.  \n",
    "* 그런데 하다 보면, RNN이 필요가 없고 Transformer 구조로 간다. RNN이 아닌, Attention의 개념으로 가게 된다.??? 이게 무엇???  \n",
    "* 그러면 우리가 매번 학습을 시켜야 하나? 그럴 필요가 없음. 이에 pre-trained(fine-tuning) 사전학습 된 것을 전이학습으로 사용하는 것으로 충분하게 된다.  \n",
    "* 여기서 나오는 것들이 BERT, GPT(챗 GPT로 알려진) 것들이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1531dc35-1c65-45be-b9e0-72a6a2ab78eb",
   "metadata": {},
   "source": [
    "* speach 음성인식 쪽은 이미 많이 개발이 되었고, 더이상 더 좋은 기술이 필요가 없을 정도이다. 라고 강사님은 본다.  \n",
    "* 그래서 굳이 speach를 진행해야할까... 라는 생각이 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87853d3-a8b9-4380-8e25-f410ba14d0c7",
   "metadata": {},
   "source": [
    "## RNN으로 가사 분석해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "731b3e82-18d8-4100-8f52-dea01406b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''저 별을 따다가 니 귀에 걸어주고파\n",
    "저 달 따다가 니 목에 걸어주고파\n",
    "세상 모든 좋은 것만 해주고 싶은\n",
    "이런 내 맘을 그댄 아나요.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2170828-61bb-434e-bf5d-9739f9b2bad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n저? -> '별을' 을 출력\\n저 별을? -> '따다가' 를 출력\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 목표\n",
    "# 위 텍스트를 학습시켰을 때\n",
    "\n",
    "'''\n",
    "저? -> '별을' 을 출력\n",
    "저 별을? -> '따다가' 를 출력\n",
    "'''\n",
    "\n",
    "# 이런 식으로, 운을 띄웠을 때 그 다음 문장을 예측하거나 추천할 수 있는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7de63e08-606e-4f9f-9c92-65104df825f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8be0569-4155-4f16-9cb6-9ea71b0d5445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'저': 1,\n",
       " '따다가': 2,\n",
       " '니': 3,\n",
       " '걸어주고파': 4,\n",
       " '별을': 5,\n",
       " '귀에': 6,\n",
       " '달': 7,\n",
       " '목에': 8,\n",
       " '세상': 9,\n",
       " '모든': 10,\n",
       " '좋은': 11,\n",
       " '것만': 12,\n",
       " '해주고': 13,\n",
       " '싶은': 14,\n",
       " '이런': 15,\n",
       " '내': 16,\n",
       " '맘을': 17,\n",
       " '그댄': 18,\n",
       " '아나요': 19}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 인덱싱\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts([text])\n",
    "tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "284c98e7-27f0-4081-b5df-b06d2930d493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab size\n",
    "vocab_size = len(tok.word_index) + 1 # +1 : zero padding용\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "62e700e9-f7f5-4f63-90e7-0672142180dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 5, 2],\n",
       " [1, 5, 2, 3],\n",
       " [1, 5, 2, 3, 6],\n",
       " [1, 5, 2, 3, 6, 4],\n",
       " [1, 7],\n",
       " [1, 7, 2],\n",
       " [1, 7, 2, 3],\n",
       " [1, 7, 2, 3, 8],\n",
       " [1, 7, 2, 3, 8, 4],\n",
       " [9, 10],\n",
       " [9, 10, 11],\n",
       " [9, 10, 11, 12],\n",
       " [9, 10, 11, 12, 13],\n",
       " [9, 10, 11, 12, 13, 14],\n",
       " [15, 16],\n",
       " [15, 16, 17],\n",
       " [15, 16, 17, 18],\n",
       " [15, 16, 17, 18, 19]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장으로 나눈 후\n",
    "# 각 문장이 전개되는 순서가 담긴 리스트 만들기\n",
    "\n",
    "seq_list = []\n",
    "\n",
    "for sentence in text.split('\\n'):\n",
    "    res = tok.texts_to_sequences([sentence])[0]\n",
    "    for i in range(1, len(res)):\n",
    "        seq = res[:i+1]\n",
    "        seq_list.append(seq)\n",
    "\n",
    "seq_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23526ab-b746-4926-b00e-3eb4a8f50a5c",
   "metadata": {},
   "source": [
    "* 위 코드 결과값 설명\n",
    "\n",
    "[1, 5], 저 별을  \n",
    "[1, 5, 2], 저 별을 따다가  \n",
    "[1, 5, 2, 3], 저 별을 따다가 니  \n",
    "[1, 5, 2, 3, 6], 저 별을 따다가 니 귀에  \n",
    "[1, 5, 2, 3, 6, 4], 저 별을 따다가 니 귀에 걸어주고파  \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "01d79005-cd64-4548-9b2e-5127e657a80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  1,  5],\n",
       "       [ 0,  0,  0,  1,  5,  2],\n",
       "       [ 0,  0,  1,  5,  2,  3],\n",
       "       [ 0,  1,  5,  2,  3,  6],\n",
       "       [ 1,  5,  2,  3,  6,  4],\n",
       "       [ 0,  0,  0,  0,  1,  7],\n",
       "       [ 0,  0,  0,  1,  7,  2],\n",
       "       [ 0,  0,  1,  7,  2,  3],\n",
       "       [ 0,  1,  7,  2,  3,  8],\n",
       "       [ 1,  7,  2,  3,  8,  4],\n",
       "       [ 0,  0,  0,  0,  9, 10],\n",
       "       [ 0,  0,  0,  9, 10, 11],\n",
       "       [ 0,  0,  9, 10, 11, 12],\n",
       "       [ 0,  9, 10, 11, 12, 13],\n",
       "       [ 9, 10, 11, 12, 13, 14],\n",
       "       [ 0,  0,  0,  0, 15, 16],\n",
       "       [ 0,  0,  0, 15, 16, 17],\n",
       "       [ 0,  0, 15, 16, 17, 18],\n",
       "       [ 0, 15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zero_padding 진행\n",
    "\n",
    "max_len = max(len(sent) for sent in seq_list)\n",
    "max_len # 결과값 : 6\n",
    "\n",
    "seq_padded = pad_sequences(seq_list, maxlen=max_len)\n",
    "seq_padded # 일반적으로 패딩 0은 값의 앞쪽에 위치한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa46ae33-4aaa-4671-9e92-37cd2d50d845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 5)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X와 y 나누기\n",
    "\n",
    "X = seq_padded[:, :-1] # X는 각 리스트에서 최종 단어 하나를 뺀 전부\n",
    "y = seq_padded[:, -1] # y는 각 리스트의 가장 마지막 값\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4928d564-0c73-4e02-ab9c-97376ffff40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 20)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y 원핫인코딩\n",
    "y_hot = to_categorical(y, num_classes = vocab_size)\n",
    "y_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "eec1f3cf-2ec6-406e-bf44-ae3d8947ae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 만들기\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 10), # 이만큼의 vocab_size가 들어갈거야, 그리고 각 단어들을 10개 차원으로 나눠서 (분석?) 해줘\n",
    "    SimpleRNN(32),\n",
    "    Dense(vocab_size, activation='softmax')]) # 원핫인코딩이므로 소프트맥스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cc60524e-8919-43ae-b241-a41cf9e1874a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, None, 10)          200       \n",
      "                                                                 \n",
      " simple_rnn_7 (SimpleRNN)    (None, 32)                1376      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 20)                660       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,236\n",
      "Trainable params: 2,236\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 둘러보기\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "be328965-cc8f-40d2-8b7a-ce6323b90465",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.4326 - accuracy: 0.9474\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4248 - accuracy: 0.9474\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4178 - accuracy: 0.9474\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4100 - accuracy: 0.9474\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.4032 - accuracy: 0.9474\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3962 - accuracy: 0.9474\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3891 - accuracy: 0.9474\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3825 - accuracy: 0.9474\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3760 - accuracy: 0.9474\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3693 - accuracy: 0.9474\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3630 - accuracy: 0.9474\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3569 - accuracy: 0.9474\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3507 - accuracy: 0.9474\n",
      "Epoch 14/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3446 - accuracy: 0.9474\n",
      "Epoch 15/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3388 - accuracy: 0.9474\n",
      "Epoch 16/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3331 - accuracy: 0.9474\n",
      "Epoch 17/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3274 - accuracy: 0.9474\n",
      "Epoch 18/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3219 - accuracy: 0.9474\n",
      "Epoch 19/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3166 - accuracy: 0.9474\n",
      "Epoch 20/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3113 - accuracy: 0.9474\n",
      "Epoch 21/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3061 - accuracy: 0.9474\n",
      "Epoch 22/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3011 - accuracy: 0.9474\n",
      "Epoch 23/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.2962 - accuracy: 0.9474\n",
      "Epoch 24/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2914 - accuracy: 0.9474\n",
      "Epoch 25/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2867 - accuracy: 0.9474\n",
      "Epoch 26/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2821 - accuracy: 0.9474\n",
      "Epoch 27/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2776 - accuracy: 0.9474\n",
      "Epoch 28/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2732 - accuracy: 0.9474\n",
      "Epoch 29/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2689 - accuracy: 0.9474\n",
      "Epoch 30/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.2648 - accuracy: 0.9474\n",
      "Epoch 31/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.2607 - accuracy: 0.9474\n",
      "Epoch 32/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2567 - accuracy: 0.9474\n",
      "Epoch 33/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.2528 - accuracy: 0.9474\n",
      "Epoch 34/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.2490 - accuracy: 0.9474\n",
      "Epoch 35/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2453 - accuracy: 0.9474\n",
      "Epoch 36/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2417 - accuracy: 0.9474\n",
      "Epoch 37/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2382 - accuracy: 0.9474\n",
      "Epoch 38/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2347 - accuracy: 0.9474\n",
      "Epoch 39/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2313 - accuracy: 0.9474\n",
      "Epoch 40/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2281 - accuracy: 0.9474\n",
      "Epoch 41/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2248 - accuracy: 0.9474\n",
      "Epoch 42/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2217 - accuracy: 0.9474\n",
      "Epoch 43/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.2187 - accuracy: 0.9474\n",
      "Epoch 44/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2157 - accuracy: 0.9474\n",
      "Epoch 45/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2128 - accuracy: 0.9474\n",
      "Epoch 46/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.2099 - accuracy: 0.9474\n",
      "Epoch 47/150\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2071 - accuracy: 0.9474\n",
      "Epoch 48/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2044 - accuracy: 0.9474\n",
      "Epoch 49/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2018 - accuracy: 0.9474\n",
      "Epoch 50/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1992 - accuracy: 0.9474\n",
      "Epoch 51/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1967 - accuracy: 0.9474\n",
      "Epoch 52/150\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.1942 - accuracy: 0.9474\n",
      "Epoch 53/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1918 - accuracy: 0.9474\n",
      "Epoch 54/150\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1895 - accuracy: 0.9474\n",
      "Epoch 55/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1872 - accuracy: 0.9474\n",
      "Epoch 56/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1850 - accuracy: 0.9474\n",
      "Epoch 57/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1828 - accuracy: 0.9474\n",
      "Epoch 58/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1806 - accuracy: 0.9474\n",
      "Epoch 59/150\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1786 - accuracy: 0.9474\n",
      "Epoch 60/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1765 - accuracy: 0.9474\n",
      "Epoch 61/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1745 - accuracy: 0.9474\n",
      "Epoch 62/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1726 - accuracy: 0.9474\n",
      "Epoch 63/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1707 - accuracy: 0.9474\n",
      "Epoch 64/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1689 - accuracy: 0.9474\n",
      "Epoch 65/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1671 - accuracy: 0.9474\n",
      "Epoch 66/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1653 - accuracy: 0.9474\n",
      "Epoch 67/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1636 - accuracy: 0.9474\n",
      "Epoch 68/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1619 - accuracy: 0.9474\n",
      "Epoch 69/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1603 - accuracy: 0.9474\n",
      "Epoch 70/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1587 - accuracy: 0.9474\n",
      "Epoch 71/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1571 - accuracy: 0.9474\n",
      "Epoch 72/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1556 - accuracy: 0.9474\n",
      "Epoch 73/150\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.1541 - accuracy: 0.9474\n",
      "Epoch 74/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1526 - accuracy: 0.9474\n",
      "Epoch 75/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1512 - accuracy: 0.9474\n",
      "Epoch 76/150\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1498 - accuracy: 0.9474\n",
      "Epoch 77/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1484 - accuracy: 0.9474\n",
      "Epoch 78/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1471 - accuracy: 0.9474\n",
      "Epoch 79/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1458 - accuracy: 0.9474\n",
      "Epoch 80/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1445 - accuracy: 0.9474\n",
      "Epoch 81/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1432 - accuracy: 0.9474\n",
      "Epoch 82/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1420 - accuracy: 0.9474\n",
      "Epoch 83/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1408 - accuracy: 0.9474\n",
      "Epoch 84/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1397 - accuracy: 0.9474\n",
      "Epoch 85/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1385 - accuracy: 0.9474\n",
      "Epoch 86/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1374 - accuracy: 0.9474\n",
      "Epoch 87/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1363 - accuracy: 0.9474\n",
      "Epoch 88/150\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1353 - accuracy: 0.9474\n",
      "Epoch 89/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1342 - accuracy: 0.9474\n",
      "Epoch 90/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1332 - accuracy: 0.9474\n",
      "Epoch 91/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1322 - accuracy: 0.9474\n",
      "Epoch 92/150\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1312 - accuracy: 0.9474\n",
      "Epoch 93/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1303 - accuracy: 0.9474\n",
      "Epoch 94/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1293 - accuracy: 0.9474\n",
      "Epoch 95/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1284 - accuracy: 0.9474\n",
      "Epoch 96/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1275 - accuracy: 0.9474\n",
      "Epoch 97/150\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.1267 - accuracy: 0.9474\n",
      "Epoch 98/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1258 - accuracy: 0.9474\n",
      "Epoch 99/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1250 - accuracy: 0.9474\n",
      "Epoch 100/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1242 - accuracy: 0.9474\n",
      "Epoch 101/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1234 - accuracy: 0.9474\n",
      "Epoch 102/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1226 - accuracy: 0.9474\n",
      "Epoch 103/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1218 - accuracy: 0.9474\n",
      "Epoch 104/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1211 - accuracy: 0.9474\n",
      "Epoch 105/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1203 - accuracy: 0.9474\n",
      "Epoch 106/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1196 - accuracy: 0.9474\n",
      "Epoch 107/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1189 - accuracy: 0.9474\n",
      "Epoch 108/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1182 - accuracy: 0.9474\n",
      "Epoch 109/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1175 - accuracy: 0.9474\n",
      "Epoch 110/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1169 - accuracy: 0.9474\n",
      "Epoch 111/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1162 - accuracy: 0.9474\n",
      "Epoch 112/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1156 - accuracy: 0.9474\n",
      "Epoch 113/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1150 - accuracy: 0.9474\n",
      "Epoch 114/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1144 - accuracy: 0.9474\n",
      "Epoch 115/150\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1138 - accuracy: 0.9474\n",
      "Epoch 116/150\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1132 - accuracy: 0.9474\n",
      "Epoch 117/150\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1126 - accuracy: 0.9474\n",
      "Epoch 118/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1120 - accuracy: 0.9474\n",
      "Epoch 119/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1115 - accuracy: 0.9474\n",
      "Epoch 120/150\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1110 - accuracy: 0.9474\n",
      "Epoch 121/150\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1104 - accuracy: 0.9474\n",
      "Epoch 122/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1099 - accuracy: 0.9474\n",
      "Epoch 123/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1094 - accuracy: 0.9474\n",
      "Epoch 124/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1089 - accuracy: 0.9474\n",
      "Epoch 125/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1084 - accuracy: 0.9474\n",
      "Epoch 126/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1079 - accuracy: 0.9474\n",
      "Epoch 127/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1075 - accuracy: 0.9474\n",
      "Epoch 128/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1070 - accuracy: 0.9474\n",
      "Epoch 129/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1066 - accuracy: 0.9474\n",
      "Epoch 130/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1061 - accuracy: 0.9474\n",
      "Epoch 131/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1057 - accuracy: 0.9474\n",
      "Epoch 132/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1053 - accuracy: 0.9474\n",
      "Epoch 133/150\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1049 - accuracy: 0.9474\n",
      "Epoch 134/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1044 - accuracy: 0.9474\n",
      "Epoch 135/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1040 - accuracy: 0.9474\n",
      "Epoch 136/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1037 - accuracy: 0.9474\n",
      "Epoch 137/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1033 - accuracy: 0.9474\n",
      "Epoch 138/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1029 - accuracy: 0.9474\n",
      "Epoch 139/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1025 - accuracy: 0.9474\n",
      "Epoch 140/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1022 - accuracy: 0.9474\n",
      "Epoch 141/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1018 - accuracy: 0.9474\n",
      "Epoch 142/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1014 - accuracy: 0.9474\n",
      "Epoch 143/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1011 - accuracy: 0.9474\n",
      "Epoch 144/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1008 - accuracy: 0.9474\n",
      "Epoch 145/150\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1004 - accuracy: 0.9474\n",
      "Epoch 146/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1001 - accuracy: 0.9474\n",
      "Epoch 147/150\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0998 - accuracy: 0.9474\n",
      "Epoch 148/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0995 - accuracy: 0.9474\n",
      "Epoch 149/150\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0992 - accuracy: 0.9474\n",
      "Epoch 150/150\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0989 - accuracy: 0.9474\n"
     ]
    }
   ],
   "source": [
    "#### 모델 컴파일\n",
    "model.compile(loss = 'categorical_crossentropy', # 로스는 어떻게 할거냐\n",
    "              optimizer = 'adam', # 로스는 어떻게 찾아갈거냐\n",
    "              metrics = ['accuracy']) # 성능 측정은 어떻게 할 거냐\n",
    "\n",
    "# 학습\n",
    "history = model.fit(X, y_hot, epochs = 150, verbose = 1)\n",
    "\n",
    "## label_encoding : sparse_categorical_crossentropy / 0.8947\n",
    "## one-hot_encoding : categorical_crossentropy / 0.9474"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ed04e9ee-97e1-408f-b0a9-bc3de3801ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e7bb173d00>]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4adf7304-3705-4c41-8687-9d8e204e1979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{15: '이런', 16: '내', 17: '맘을', 18: '그댄'}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측하고 싶은 텍스트 인코딩\n",
    "start_text = '이런 내 맘을 그댄'\n",
    "encoded = tok.texts_to_sequences([start_text])\n",
    "encoded_dic = { y : x for x, y in zip(start_text.split(), encoded[0])}\n",
    "\n",
    "\n",
    "# 예측하고 싶은 텍스트 패딩\n",
    "padded = pad_sequences(encoded, maxlen = max_len)\n",
    "padded\n",
    "\n",
    "## 패딩하는 이유 : 다시 한 번 상기해보자면, 입력값의 모양이 일정해야 분석이 가능하기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8ebcc7b7-f6c3-47b1-a3ea-50fe56485d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 52ms/step\n"
     ]
    }
   ],
   "source": [
    "# 예측\n",
    "res = model.predict(padded, verbose=1)\n",
    "tok.word_index.get(res.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4710ae-fded-4179-a1d5-cd4a2b49aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마무리\n",
    "## 임베딩\n",
    "## RNN을 이용한 학습\n",
    "## Dense 학습 후 출력\n",
    "\n",
    "\n",
    "## 반댓말, 동의어 등 유사도 찾기는 : word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bdf8c1-818c-41c0-8e73-98080f22a3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
