{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f46cea-c8c4-4b18-9705-5d89f4603893",
   "metadata": {},
   "source": [
    "# ■■■ 설치 파일 및 패키지 소개\n",
    "\n",
    "> <b>설치 파일 및 패키지</b>  \n",
    "> * jpype : [강사님이 배포한 설치파일](https://mybox.naver.com/share/list?shareKey=IV_zizC9cHRFLU_3DTGPI_O6ck2ITY63cAx9O4kwz9kA)  \n",
    "> * konlpy : `pip install konlpy`  \n",
    "  \n",
    "> <b>konlpy</b>  \n",
    "> 한국어 정보처리를 위한 파이썬 패키지  \n",
    "> 한글 형태소 분석기라고도 불린다.  \n",
    "> [https://konlpy.org/ko/latest/index.html](https://konlpy.org/ko/latest/index.html)  \n",
    "> - 자바 jdk가 설치되어있어야 함  \n",
    "\n",
    "> <b>jpype</b>  \n",
    "> - java와 파이썬이 서로 연동될 수 있게 해준다.  \n",
    "> - jpype는 사용중인 파이썬 버전에 맞는 버전으로 설치해야 함  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f4f94b-6abf-4b9b-b650-73eeab294894",
   "metadata": {},
   "source": [
    "# ■■■ 에러"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b95d20f-b755-4b0e-ad45-c6ba4f49c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. jvm.dll 파일을 찾을 수 없음\n",
    "## 에러메세지 : No JVM shared library file (jvm.dll) found. Try setting up the JAVA_HOME environment variable properly.\n",
    "## 찾아보니, 자바 버전이 맞지 않아 나오는 문제\n",
    "## (1) java18버전을 새로 설치해준 후\n",
    "## (2) 환경변수에서 기존 자바 버전 폴더 삭제 후 새 자바 버전이 설치된 경로를 환경변수에 추가\n",
    "## (3) 위 Path에도 새로 설치된 자바 버전 경로를 추가함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f0c6ed-d8e9-44d1-b08e-b53a7e273b16",
   "metadata": {},
   "source": [
    "# ■■■ 강사님의 생각"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90af7aa7-c62e-4191-ba8b-13a2705ef801",
   "metadata": {},
   "source": [
    "## accuracy를 높여라??  \n",
    "* 자신은 accuracy를 그렇게 중요하게 생각하지는 않는다.  \n",
    "* 예를 들어, fashion_mnist에서 샌들을 신발로 분류했다면, 그게 심각한 문제인가?  \n",
    "* 그럴 수도 있다라고 넘어갈 수 있는 문제라고 생각한다.  \n",
    "* accuracy를 높이는 작업은 결국 과적합으로 가는 길이 아닌가 라고 본인은 생각한다.  \n",
    "* 중요한 것은 실제 제공하는 서비스에 문제가 있는가, 기능에서 이게 문제가 되는 것인가 라고 생각한다.  \n",
    "* 오히려 데이터를 까보고, '실질적인 내용' 즉, 더 좋은 서비스가 되는 방향에 대해 생각하는 게 맞다고 생각한다.  \n",
    "\n",
    "## 미안해하는 것??  \n",
    "* 미안해하면 상대방도 나에게 미안할 일을 하지 못한다.  \n",
    "* 그것보다는 서로 더 잘 부탁을 하도록 쉽게 접근하고, 쉽게 해결하고, 좋은 분위기로 갈 수 있도록 하는  \n",
    "* 유연한 분위기가 되는 게 상당히 중요하다.  \n",
    "* 이건 내 몫의 일이야, 이걸 내가 감당해야해 라고 하는 것은 올드한 사고방식이다.  \n",
    "\n",
    "## 대중에게 공표하는 행위  \n",
    "* 할까 말까 생각중이라면 우선 해봐라.  \n",
    "* 선생님은 밴드를 하면서 곡들을 6곡 정도 냈다.  \n",
    "* 본인은 완성도가 떨어지고 내놓기에 쑥쓰러운 점이 있었으나, 친구의 \"이때 안내면 나중에도 안낸다\"라는 이야기에 냈다고 한다.  \n",
    "* 잘했다고 생각하는 중. 소소하지만 수입도 있고, 뜻밖의 컨택을 받을 수도 있다고 한다.  \n",
    "\n",
    "june2it@kaist.ac.kr\n",
    "먼데이모닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4cbc3d-d6d5-4498-92ed-fbd9ddf0e28a",
   "metadata": {},
   "source": [
    "# ■■■ 숙제\n",
    "* 맥북에 형태소 분석기 설치  \n",
    "* 람다함수 사용법 공부  \n",
    "* 원핫인코딩 get dummies 공부하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc25d8b-6813-4b90-baa4-1c38496b4d56",
   "metadata": {},
   "source": [
    "# ■■■ 수업 듣고 생각"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8abebb4-4abd-4513-9a23-c0e29f4971f2",
   "metadata": {},
   "source": [
    "* 형태소 분석기는 다양한 종류가 있다.  \n",
    "* 다 그 각각의 기능이 있으나, 이 모두를 외울 필요는 없고  \n",
    "* 아 이런 게 있구나~ 정도의 익히기만 해두고, 나중에 필요할 때 찾아 쓸 수 있을 정도로만 정리해두면 되겠다.  \n",
    "\n",
    "\n",
    "* noun으로 분리하면 편리하기는 하지만, 형용사 등 의미가 있을 수 있는 단어를 자르는 경향도 있다.  \n",
    "* 그러므로 morph로 분리하여 분석하는 것을 추천하며, noun은 시각화 등에서 주로 사용하는 게 좋다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3902f77d-6254-4631-84b7-257bff3c49c8",
   "metadata": {},
   "source": [
    "# nltk 영문 형태소 분석기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa4ec0b-60c1-49a9-8abd-198be5a4321b",
   "metadata": {},
   "source": [
    "* 여러 방식의 형태소 분석기가 있으며, 상황에 맞게 사용하면 된다.  \n",
    "* 또한, 분석기에 따라 처리 속도가 다르므로 이 또한 상황에 맞게 사용해야 한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a20ca1e9-05fc-4d57-b235-8f8e05ac7ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "238fb22f-830d-437f-84a0-5da5ffb76dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f2ce8e3-62c5-40f7-89af-a9d95f0bfaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\"Don't This is the toughest operation I've ever seen. The enemy has thrown its strongest assault at Bakhmut.\n",
    "We haven't seen troops like this before,\" the Ukrainian commander tells us.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c81c78d3-9fc5-4645-bd97-42016dc69aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'Do', \"n't\", 'This', 'is', 'the', 'toughest', 'operation', 'I', \"'ve\", 'ever', 'seen', '.', 'The', 'enemy', 'has', 'thrown', 'its', 'strongest', 'assault', 'at', 'Bakhmut', '.', 'We', 'have', \"n't\", 'seen', 'troops', 'like', 'this', 'before', ',', \"''\", 'the', 'Ukrainian', 'commander', 'tells', 'us', '.']\n"
     ]
    }
   ],
   "source": [
    "# nltk 형태소 분석 다른유형 - 1\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "280c1e2b-0534-4ce1-85c2-2c8ae4e93c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"', 'Don', \"'\", 't', 'This', 'is', 'the', 'toughest', 'operation', 'I', \"'\", 've', 'ever', 'seen', '.', 'The', 'enemy', 'has', 'thrown', 'its', 'strongest', 'assault', 'at', 'Bakhmut', '.', 'We', 'haven', \"'\", 't', 'seen', 'troops', 'like', 'this', 'before', ',\"', 'the', 'Ukrainian', 'commander', 'tells', 'us', '.']\n"
     ]
    }
   ],
   "source": [
    "# nltk 형태소 분석 다른유형 - 2\n",
    "print(WordPunctTokenizer().tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "217aa20b-99f3-4047-821d-849f9b4fa10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Don\\'t This is the toughest operation I\\'ve ever seen.',\n",
       " 'The enemy has thrown its strongest assault at Bakhmut.',\n",
       " 'We haven\\'t seen troops like this before,\" the Ukrainian commander tells us.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk 문장 단위로 끊기\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bde8b2d-e7a1-4596-b85e-5403e3fa3233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'this', 'is', 'the', 'toughest', 'operation', \"i've\", 'ever', 'seen', 'the', 'enemy', 'has', 'thrown', 'its', 'strongest', 'assault', 'at', 'bakhmut', 'we', \"haven't\", 'seen', 'troops', 'like', 'this', 'before', 'the', 'ukrainian', 'commander', 'tells', 'us']\n"
     ]
    }
   ],
   "source": [
    "# tensorflow 형태소 분석 - 문장 구분\n",
    "print(text_to_word_sequence(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f4dc40-4801-4803-9908-3c6599a893e3",
   "metadata": {},
   "source": [
    "# konlpy 한글 형태소 분석기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dceaf96d-3105-49e8-a228-34e9057644d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import konlpy\n",
    "konlpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0f5b39f-f274-43f5-b9f2-5eb46a47fa05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '는', '자연어', '처리', '를', '배우고', '있어요', ',', '너무', '신기해요']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "tokens = okt.morphs(\"나는 자연어 처리를 배우고 있어요, 너무 신기해요\")\n",
    "tokens\n",
    "# 형태소 분석기는 조사 등을 제외한 단어만을 추출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b3797c6-c9ec-4a52-b9c2-90a24973112b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나는', '자연어', '처리를', '배우고', '있어요,', '너무', '신기해요']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"나는 자연어 처리를 배우고 있어요, 너무 신기해요\".split()\n",
    "# 형태소 분석기와는 다른 결과를 내고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2317cd49-29c2-4b8b-9577-dff9f622e6cd",
   "metadata": {},
   "source": [
    "# kss 형태소 분석기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f9cf2e3-f8cb-43bd-bc21-9a6692e3da3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''딥러닝 자연어 처리는 흥미롭습니다. 그런데 재미는 없을 수도 있습니다.\n",
    "특히 일상 언어는 너무 복잡합니다.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a2a664b-8dfe-4ce9-a444-d6da90cac17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
      "For your information, Kss also supports mecab backend.\n",
      "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
      "Please refer to following web sites for details:\n",
      "- mecab: https://cleancode-ws.tistory.com/97\n",
      "- konlpy.tag.Mecab: https://uwgdqo.tistory.com/363\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('딥', 'NNG'), ('러닝', 'NNG'), ('자연어', 'NNG'), ('처리', 'NNG'), ('는', 'JX'), ('흥미', 'NNG'), ('롭', 'XSA'), ('습니다', 'EF'), ('.', 'SF'), ('그런데', 'MAJ'), ('재미', 'NNG'), ('는', 'JX'), ('없', 'VA'), ('을', 'ETM'), ('수', 'NNB'), ('도', 'JX'), ('있', 'VA'), ('습니다', 'EF'), ('.', 'SF'), ('특히', 'MAG'), ('일상', 'NNG'), ('언어', 'NNG'), ('는', 'JX'), ('너무', 'MAG'), ('복잡', 'XR'), ('합니다', 'XSA+EF'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "print(kss.split_morphemes(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d3a0399-f142-4334-8f78-7f1886af80ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['딥러닝 자연어 처리는 흥미롭습니다.', '그런데 재미는 없을 수도 있습니다.', '특히 일상 언어는 너무 복잡합니다.']\n"
     ]
    }
   ],
   "source": [
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a16892c8-105a-4fca-8c94-ea780db8a9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['딥러닝 자연어 처리는 흥미롭습니다.', '그런데 재미는 없을 수도 있습니다.', '특히 일상 언어는 너무 복잡합니다.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kss.summarize_sentences(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39328300-ac2b-42d0-a107-2602812de026",
   "metadata": {},
   "source": [
    "# Kkma 꼬꼬마 형태소 분석기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5828e8c2-8017-440a-8d8b-1ffaa4a3da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5847963-39d0-4884-a110-4aab66a2ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98a76488-5240-4444-b9a1-d0e0a5ee7283",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''딥러닝 자연어 처리는 흥미롭습니다. 그런데 재미는 없을 수도 있습니다.\n",
    "특히 일상 언어는 너무 복잡합니다.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c70c2351-1e52-47d0-83d0-ffe442831147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['딥', '러닝', '자연어', '처리', '는', '흥미롭', '습니다', '.', '그런데', '재미', '는', '없', '을', '수', '도', '있', '습니다', '.', '특히', '일상', '언어', '는', '너무', '복잡', '하', 'ㅂ니다', '.']\n"
     ]
    }
   ],
   "source": [
    "print(kkma.morphs(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6cf92cc-7346-416b-b57b-4e6e9c644b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('딥', 'NNG'), ('러닝', 'NNG'), ('자연어', 'NNG'), ('처리', 'NNG'), ('는', 'JX'), ('흥미롭', 'VA'), ('습니다', 'EFN'), ('.', 'SF'), ('그런데', 'MAG'), ('재미', 'NNG'), ('는', 'JX'), ('없', 'VA'), ('을', 'ETD'), ('수', 'NNB'), ('도', 'JX'), ('있', 'VV'), ('습니다', 'EFN'), ('.', 'SF'), ('특히', 'MAG'), ('일상', 'NNG'), ('언어', 'NNG'), ('는', 'JX'), ('너무', 'MAG'), ('복잡', 'XR'), ('하', 'XSA'), ('ㅂ니다', 'EFN'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "print(kkma.pos(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "477c67b4-54c8-4d0e-99e1-c8fcdc81eeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['딥', '딥러닝', '러닝', '자연어', '처리', '재미', '수', '일상', '언어']\n"
     ]
    }
   ],
   "source": [
    "print(kkma.nouns(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba149ed-30aa-420e-b380-7898e0b005f0",
   "metadata": {},
   "source": [
    "# 형태소 분석기 비교해보기 Okt vs Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72bf3f8e-cf10-4759-b836-d3e347f99b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt, Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acd247e3-774d-423d-bcc4-15d06d2ff838",
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d271ddc-c479-4523-983f-ef18ca9e4b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Okt ====\n",
      "형태소 분석 :  ['열심히', '코딩', '한', '당신', ',', '잠도', '잘', '자고', '일', '하세요', '.']\n",
      "품사 태깅 :  [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('잠도', 'Noun'), ('잘', 'Verb'), ('자고', 'Noun'), ('일', 'Noun'), ('하세요', 'Verb'), ('.', 'Punctuation')]\n",
      "명사 분석 :  ['코딩', '당신', '잠도', '자고', '일']\n",
      "==== Kkma ====\n",
      "형태소 분석 :  ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '잠', '도', '잘', '자', '고', '일하', '세요', '.']\n",
      "품사 태깅 :  [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('잠', 'NNG'), ('도', 'JX'), ('잘', 'MAG'), ('자', 'VV'), ('고', 'ECE'), ('일하', 'VV'), ('세요', 'EFN'), ('.', 'SF')]\n",
      "명사 분석 :  ['코딩', '당신', '잠']\n"
     ]
    }
   ],
   "source": [
    "text = '열심히 코딩한 당신, 잠도 잘 자고 일하세요.'\n",
    "\n",
    "print('==== Okt ====')\n",
    "print('형태소 분석 : ', okt.morphs(text)) #단어들\n",
    "print('품사 태깅 : ', okt.pos(text)) #품사만\n",
    "print('명사 분석 : ', okt.nouns(text)) #명사만\n",
    "\n",
    "print('==== Kkma ====')\n",
    "print('형태소 분석 : ', kkma.morphs(text)) #단어들\n",
    "print('품사 태깅 : ', kkma.pos(text)) #품사만\n",
    "print('명사 분석 : ', kkma.nouns(text)) #명사만"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b006e6-537d-4859-9b6e-112e31b4818a",
   "metadata": {},
   "source": [
    "# pykospacing : 한글 띄어쓰기 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "667fcff0-4946-4931-b732-897ff0989278",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''제네시스BBQ가 대졸 신입사원의 연봉을 33.5% 인상한다고 4일 밝혔다. 지난해 3400만원이었던 대졸 초임의 연봉이 4540만원으로 오른다. 치킨 프랜차이즈 업계의 평균(3300만원)과 비교했을 때 최고 수준이다. 우수한 인재를 확보해 글로벌 경기침체를 극복하겠다는 ‘신(新) 인재경영’의 일환이다.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61fda810-9dcf-4159-8e45-94213772baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 빈칸을 강제로 없애보기\n",
    "new_text = text.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfb2ab9c-8dbb-4c46-8010-cef4a562be43",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/haven-jeon/PyKoSpacing\n",
      "  Cloning https://github.com/haven-jeon/PyKoSpacing to c:\\users\\user\\appdata\\local\\temp\\pip-req-build-o2u1lvwv\n",
      "  Resolved https://github.com/haven-jeon/PyKoSpacing to commit 1f8d11c59ac93525432f164e4c237ece4e298691\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: tensorflow==2.9.3 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from pykospacing==0.5) (2.9.3)\n",
      "Requirement already satisfied: h5py==3.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from pykospacing==0.5) (3.1.0)\n",
      "Collecting argparse>=1.4.0\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: numpy>=1.17.5 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from h5py==3.1.0->pykospacing==0.5) (1.23.5)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorflow==2.9.3->pykospacing==0.5) (1.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorflow==2.9.3->pykospacing==0.5) (1.6.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorflow==2.9.3->pykospacing==0.5) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorflow==2.9.3->pykospacing==0.5) (1.16.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorflow==2.9.3->pykospacing==0.5) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorflow==2.9.3->pykospacing==0.5) (0.29.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorflow==2.9.3->pykospacing==0.5) (3.19.6)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.9.3->pykospacing==0.5) (2.9.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.9.3->pykospacing==0.5) (2.9.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.9.3->pykospacing==0.5) (2.9.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorflow==2.9.3->pykospacing==0.5) (2.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorflow==2.9.3->pykospacing==0.5) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorflow==2.9.3->pykospacing==0.5) (1.1.2)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorflow==2.9.3->pykospacing==0.5) (0.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorflow==2.9.3->pykospacing==0.5) (14.0.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorflow==2.9.3->pykospacing==0.5) (4.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorflow==2.9.3->pykospacing==0.5) (1.42.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.9.3->pykospacing==0.5) (1.12)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorflow==2.9.3->pykospacing==0.5) (22.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorflow==2.9.3->pykospacing==0.5) (0.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from astunparse>=1.6.0->tensorflow==2.9.3->pykospacing==0.5) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.3->pykospacing==0.5) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.3->pykospacing==0.5) (2.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.3->pykospacing==0.5) (2.6.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.3->pykospacing==0.5) (0.4.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.3->pykospacing==0.5) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.3->pykospacing==0.5) (2.28.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.3->pykospacing==0.5) (3.4.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.3->pykospacing==0.5) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.3->pykospacing==0.5) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.3->pykospacing==0.5) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.3->pykospacing==0.5) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.3->pykospacing==0.5) (6.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.3->pykospacing==0.5) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.3->pykospacing==0.5) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.3->pykospacing==0.5) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.3->pykospacing==0.5) (2.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.3->pykospacing==0.5) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.3->pykospacing==0.5) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.3->pykospacing==0.5) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.3->pykospacing==0.5) (3.2.1)\n",
      "Installing collected packages: argparse\n",
      "Successfully installed argparse-1.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/haven-jeon/PyKoSpacing 'C:\\Users\\user\\AppData\\Local\\Temp\\pip-req-build-o2u1lvwv'\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/haven-jeon/PyKoSpacing --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27c87a75-a088-47bf-907e-6e5dd391baef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykospacing import Spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ab779ce-2494-465c-87cb-2e3676ac9f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "■■■ 기존 텍스트 : \n",
      " 제네시스BBQ가 대졸 신입사원의 연봉을 33.5% 인상한다고 4일 밝혔다. 지난해 3400만원이었던 대졸 초임의 연봉이 4540만원으로 오른다. 치킨 프랜차이즈 업계의 평균(3300만원)과 비교했을 때 최고 수준이다. 우수한 인재를 확보해 글로벌 경기침체를 극복하겠다는 ‘신(新) 인재경영’의 일환이다.\n",
      "■■■ 띄어쓰기 없앤 텍스트 : \n",
      " 제네시스BBQ가대졸신입사원의연봉을33.5%인상한다고4일밝혔다.지난해3400만원이었던대졸초임의연봉이4540만원으로오른다.치킨프랜차이즈업계의평균(3300만원)과비교했을때최고수준이다.우수한인재를확보해글로벌경기침체를극복하겠다는‘신(新)인재경영’의일환이다.\n",
      "■■■ pykospacing으로 띄어쓰기 : \n",
      " 제네시스 BBQ가 대졸 신입사원의 연봉을 33.5% 인상한다고 4일 밝혔다. 지난해 3400만원이었던 대졸 초임의 연봉이 4540만원으로 오른다. 치킨 프랜차이즈 업계의 평균(3300만원)과 비교했을 때 최고 수준이다. 우수한 인재를 확보해 글로벌 경기침체를 극복하겠다는 ‘신(新)인재경영’의 일환이다.\n"
     ]
    }
   ],
   "source": [
    "s = Spacing()\n",
    "recon_text = s(new_text)\n",
    "print(\"■■■ 기존 텍스트 : \\n\", text)\n",
    "print(\"■■■ 띄어쓰기 없앤 텍스트 : \\n\", new_text)\n",
    "print(\"■■■ pykospacing으로 띄어쓰기 : \\n\", recon_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "969c36ac-c2e9-41ec-929d-2ee57c26f4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykospacing import kospacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7deb3693-ad03-4635-97a8-c94ff6c1eaf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[43mkospacing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "kospacing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97d7b17-c25f-4a9f-ab8f-76fc324840c4",
   "metadata": {},
   "source": [
    "# py-hanspell 한글 맞춤법 교정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4bd45809-181a-4a9c-81ce-cb2eaa937d96",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ssut/py-hanspell.git\n",
      "  Cloning https://github.com/ssut/py-hanspell.git to c:\\users\\user\\appdata\\local\\temp\\pip-req-build-c2x0y2m5\n",
      "  Resolved https://github.com/ssut/py-hanspell.git to commit 8e993cf46f97f9d665c15633a0fc78ac1b727713\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from py-hanspell==1.1) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from requests->py-hanspell==1.1) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from requests->py-hanspell==1.1) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from requests->py-hanspell==1.1) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from requests->py-hanspell==1.1) (2022.12.7)\n",
      "Building wheels for collected packages: py-hanspell\n",
      "  Building wheel for py-hanspell (setup.py): started\n",
      "  Building wheel for py-hanspell (setup.py): finished with status 'done'\n",
      "  Created wheel for py-hanspell: filename=py_hanspell-1.1-py3-none-any.whl size=4877 sha256=985d7ab9bd83ae941649b21255002e064dad1dc12ff619ed7c20c52f27dd2ef5\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-ciazs5tw\\wheels\\3f\\a5\\73\\e4d2806ae141d274fdddaabf8c0ed79be9357d36bfdc99e4b4\n",
      "Successfully built py-hanspell\n",
      "Installing collected packages: py-hanspell\n",
      "Successfully installed py-hanspell-1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/ssut/py-hanspell.git 'C:\\Users\\user\\AppData\\Local\\Temp\\pip-req-build-c2x0y2m5'\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/ssut/py-hanspell.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e538daf-d939-4c1f-b3fc-e2eea373a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hanspell import spell_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d131562-b088-4bbb-a368-a7918af4c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_x = \"마춤법 틀리면 외 않됀당거냐? 내맘대로 쓰면돼지!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c60c9756-aa57-4fa4-8dea-288721ae1e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'맞춤법 틀리면 외 않됀당거냐? 내 맘대로 쓰면 되지!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ok = spell_checker.check(text_x)\n",
    "text_ok.checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dbdb3d9-4422-4a0d-ad74-38204f7b58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_x = \"마춤법 틀리면 외 안된단거냐? 내맘대로 쓰면돼지!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5fa8b20-ab2d-4e00-a335-8e95deb604cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'맞춤법 틀리면 외 안 된단 거냐? 내 맘대로 쓰면 되지!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ok = spell_checker.check(text_x)\n",
    "text_ok.checked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe313b5-4116-41fd-a1fa-4c16d01c7560",
   "metadata": {},
   "source": [
    "# soynlp \n",
    "* 본 패키지의 normalizer는 대화 데이터, 댓글 데이터에 등장하는 반복되는 이모티콘의 정리 및 한글, 혹은 텍스트만 남기기 위한 함수를 제공합니다.\n",
    "https://github.com/lovit/soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d516609-aaff-47b0-9fe9-8ccd86987af7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soynlp\n",
      "  Downloading soynlp-0.0.493-py3-none-any.whl (416 kB)\n",
      "     -------------------------------------- 416.8/416.8 kB 8.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.12.1 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from soynlp) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from soynlp) (1.9.3)\n",
      "Collecting scikit-learn>=0.20.0\n",
      "  Downloading scikit_learn-1.2.0-cp38-cp38-win_amd64.whl (8.2 MB)\n",
      "     ---------------------------------------- 8.2/8.2 MB 17.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: psutil>=5.0.1 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from soynlp) (5.9.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages (from scikit-learn>=0.20.0->soynlp) (1.2.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn, soynlp\n",
      "Successfully installed scikit-learn-1.2.0 soynlp-0.0.493 threadpoolctl-3.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eba02a54-c05f-47cb-9e7e-a104f1779bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp import normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0391c5d1-c268-44ca-8607-a04e24c2539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"앜ㅋㅋㅋㅋㅋ이김밥존맛탱ㅋㅋㅋㅋㅋ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "963d6c3c-9f4c-47c5-be19-878baf86140e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아ㅋㅋ김밥존맛ㅋㅋ'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer.emoticon_normalize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8a95277-0d25-480c-9610-0416aeff09db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아ㅋㅋㅋ김밥존맛ㅋㅋㅋ'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer.emoticon_normalize(text, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93ad3db9-b154-4685-a8bd-c6c51834672b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아ㅋ김밥존맛ㅋ'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer.emoticon_normalize(text, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88544ec5-437c-4f04-8512-57d2893885f1",
   "metadata": {},
   "source": [
    "# Stop Words : 불용어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459ccb10-0a1a-43ef-a3aa-26c73c4e5434",
   "metadata": {},
   "source": [
    "## nltk에서 불용어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acd397e2-8b54-4df2-877d-d7ba167dbf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "005e8564-6968-4bfb-ad12-bc258a67d66d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# 불용어 사전 둘러보기\n",
    "stop_words_list = stopwords.words(\"english\")\n",
    "print(stop_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1092a7f3-9b30-4860-8d05-4eb365e9ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '''Other leaders will be there in unofficial capacities - \n",
    "including King Philippe of Belgium and Queen Letizia of Spain, as well as the leaders of Poland and Hungary, the Catholic news agency reports.'''.lower()\n",
    "# 불용어사전에는 단어들이 모두 소문자로 등록되어있기 때문에, 소문자처리를 해주었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c260ce78-1169-4aa4-8035-106cd59ba132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 :  ['other', 'leaders', 'will', 'be', 'there', 'in', 'unofficial', 'capacities', '-', 'including', 'king', 'philippe', 'of', 'belgium', 'and', 'queen', 'letizia', 'of', 'spain', ',', 'as', 'well', 'as', 'the', 'leaders', 'of', 'poland', 'and', 'hungary', ',', 'the', 'catholic', 'news', 'agency', 'reports', '.']\n",
      "불용어 제거 :  ['leaders', 'unofficial', 'capacities', '-', 'including', 'king', 'philippe', 'belgium', 'queen', 'letizia', 'spain', ',', 'well', 'leaders', 'poland', 'hungary', ',', 'catholic', 'news', 'agency', 'reports', '.']\n"
     ]
    }
   ],
   "source": [
    "token_1 = word_tokenize(test)\n",
    "token_2 = [x for x in token_1 if x not in stop_words_list]\n",
    "print(\"원본 : \", token_1)\n",
    "print(\"불용어 제거 : \", token_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f2d5e-8706-4796-882e-6bf893e5f169",
   "metadata": {},
   "source": [
    "## 불용어 사전에 내가 원하는 단어가 없다면..?\n",
    "-> 불용어 사전을 만들어주면 된다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a97ae0df-750e-4224-8e31-f1b5f337ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = ['fuck', 'ass', 'hole', 'fucker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bfa61fa1-317a-4180-b0cf-4923bd2e3803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 :  ['drop', 'your', 'weapon', '.', 'fuck', 'you', 'ass', 'hole', '.', 'go', 'the', 'hell', 'mother', 'fucker']\n",
      "1차 정제 :  ['drop', 'weapon', '.', 'fuck', 'ass', 'hole', '.', 'go', 'hell', 'mother', 'fucker']\n",
      "2차 정제 :  ['drop', 'weapon', '.', '.', 'go', 'hell', 'mother']\n"
     ]
    }
   ],
   "source": [
    "test = \"Drop your weapon. Fuck you ass hole. go the hell mother fucker\".lower()\n",
    "token_1 = word_tokenize(test)\n",
    "token_2 = [x for x in token_1 if x not in stop_words_list]\n",
    "token_3 = [x for x in token_2 if x not in my_stop_words]\n",
    "\n",
    "print(\"원문 : \", token_1)\n",
    "print(\"1차 정제 : \", token_2)\n",
    "print(\"2차 정제 : \", token_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56a9117-1caf-42d9-ae34-6e53e4024932",
   "metadata": {},
   "source": [
    "## 한글 불용어 사전"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddfc541-0a5d-41ed-a754-bdbc3265a182",
   "metadata": {},
   "source": [
    "Okt혹은 kkma 등 라이브러리엔 아직까지 딱히 불용어 사전이 없다.  \n",
    "때문에 불용어 사전을 직접 만들어서 써줘야 한다.  \n",
    "\n",
    "여러 방법이 있겠지만, 공통적으로 진행하는 사항은  \n",
    "(1) 우선 한 글자 단어를 뺀다. 한 글자 짜리 단어는 보통 의미를 가지고 있는 경우가 많지 않다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "58b577fc-ada9-4303-a022-dfe2b80c37c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4cdfa573-5683-4870-baee-f5a23a446fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"이 따위 물건을 팔고도 돈을 쳐먹냐. 그냥 줘도 아깝다. 하자 있는 물건을 어떻게 쓰냐. 병신새끼야 ㅂㅅ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "737e00f6-2bd6-4183-979d-ca073a3cfa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 :  ['이', '따위', '물건', '을', '팔고', '도', '돈', '을', '쳐', '먹냐', '.', '그냥', '줘도', '아깝다', '.', '하자', '있는', '물건', '을', '어떻게', '쓰냐', '.', '병신', '새끼', '야', 'ㅂㅅ']\n",
      "1차 정제 :  ['따위', '물건', '팔고', '먹냐', '그냥', '줘도', '아깝다', '하자', '있는', '물건', '어떻게', '쓰냐', '병신', '새끼', 'ㅂㅅ']\n",
      "최종 정제 :  ['따위', '물건', '팔고', '먹냐', '그냥', '줘도', '아깝다', '하자', '있는', '물건', '어떻게', '쓰냐']\n"
     ]
    }
   ],
   "source": [
    "# 접근법 1 : 이, 을 등 한글자를 뻬기\n",
    "\n",
    "token_ko_1 = okt.morphs(text)\n",
    "token_ko_2 = [x for x in token_ko_1 if len(x) > 1]\n",
    "my_stop_words = ['병신', '새끼', 'ㅄ', 'ㅂㅅ', 'ㅂㅕㅇㅅㅣㄴ'] # 나의 불용어 사전 만들기\n",
    "token_ko_3 = [x for x in token_ko_2 if x not in my_stop_words]\n",
    "\n",
    "print(\"원문 : \", token_ko_1)\n",
    "print(\"1차 정제 : \", token_ko_2)\n",
    "print(\"최종 정제 : \", token_ko_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2cb2c8bf-2f1e-42e8-8ee1-bd3468511df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 :  ['이', '따위', '물건', '팔고', '돈', '그냥', '하자', '물건', '병신', '새끼']\n",
      "최종 정제 :  ['이', '따위', '물건', '팔고', '돈', '그냥', '하자', '물건']\n"
     ]
    }
   ],
   "source": [
    "# 접근법 2 : noun 명사만 뽑아내기\n",
    "\n",
    "token_ko_1 = okt.nouns(text)\n",
    "my_stop_words = ['병신', '새끼', 'ㅄ', 'ㅂㅅ', 'ㅂㅕㅇㅅㅣㄴ'] # 나의 불용어 사전 만들기\n",
    "token_ko_2 = [x for x in token_ko_1 if x not in my_stop_words]\n",
    "\n",
    "print(\"원문 : \", token_ko_1)\n",
    "print(\"최종 정제 : \", token_ko_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1d79f0-748b-41d5-bfa8-a51cc84b1d9c",
   "metadata": {},
   "source": [
    "# ■■■ 여기서 분리 필요 ■■■"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8da5270-5b8d-42a0-b102-d5c90978c6a2",
   "metadata": {},
   "source": [
    "# 정수 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3e65df-0698-41b7-a22c-bfd349f8bff2",
   "metadata": {},
   "source": [
    "컴퓨터는 숫자로 소통하고 계산을 한다.  \n",
    "이를 뒤집어 이야기해보면, 텍스트는 컴퓨터가 이해하거나 다룰 수 있는 기호가 아니라는 것.  \n",
    "이 때문에, 텍스트를 컴퓨터가 인식하고 계산하게끔 하기 위해서는 '인코딩'이 필요하다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27abfc41-dd94-430d-b35b-8d50b22b8c2c",
   "metadata": {},
   "source": [
    "## 아스키 코드 ASCII CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f7509f-d7a1-49fc-9673-11986ec11819",
   "metadata": {},
   "source": [
    "ASCII( /ˈæski/, 아스키)는 영문 알파벳을 사용하는 대표적인 문자 인코딩이다.  \n",
    "아스키는 컴퓨터와 통신 장비를 비롯한 문자를 사용하는 많은 장치에서 사용되며, 대부분의 문자 인코딩이 아스키에 기초를 두고 있다.\n",
    "\n",
    "https://ko.wikipedia.org/wiki/ASCII"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4a0115-080d-4998-8e90-69e19a2d70bb",
   "metadata": {},
   "source": [
    "## 데이터 분석에서의 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58261f65-b243-4ed0-b4aa-71c892015e76",
   "metadata": {},
   "source": [
    "하지만 여러 언어의 모든 단어들, 문장들을 사전에 약속해 인코딩한다는 것은 불가능에 가깝다.  \n",
    "그러므로 어떠한 프로젝트가 있을 때마다 인코딩을 진행해주게 된다.  \n",
    "\n",
    "★★인코딩시에는 index 번호를 0은 비워두게 된다.★★  \n",
    "이는 CNN에서의 padding과 비슷하다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914a2be9-2a54-4619-ac01-25ee3b594eb2",
   "metadata": {},
   "source": [
    "## 말뭉치란"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537e7062-ce60-44a1-b719-3f8f607741a8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "* 컴퓨터가 분석할 수 있게 만들어진 단어의 집합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f70bd67-00d0-4dd1-a986-815a59628494",
   "metadata": {},
   "source": [
    "## 가사 인코딩해보기 (하드코딩)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570c7f8b-def4-46ac-9d41-206364d0c350",
   "metadata": {},
   "source": [
    "* 반복적인 단어, 자주 등장하는 단어는 중요도가 높다고 보여지는 게 일반적이며,  \n",
    "* 딕셔너리(인덱스 리스트)를 만들 때 자주 등장하는 단어를 앞쪽에 배치하는 게 일반적이다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c894d97e-2b51-4853-876d-f5b76b6695da",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Woo woo woo woo ooh\n",
    "Woo woo woo woo\n",
    "Stay in the middle\n",
    "Like you a little\n",
    "Don’t want no riddle\n",
    "말해줘 say it back\n",
    "Oh say it ditto\n",
    "아침은 너무 멀어\n",
    "So say it ditto\n",
    "훌쩍 커버렸어\n",
    "함께한 기억처럼\n",
    "널 보는 내 마음은\n",
    "어느새 여름 지나 가을\n",
    "기다렸지 all this time\n",
    "Do you want somebody\n",
    "Like I want somebody\n",
    "날 보고 웃었지만\n",
    "Do you think about me now yeah\n",
    "All the time yeah\n",
    "All the time\n",
    "I got no time to lose\n",
    "내 길었던 하루\n",
    "난 보고 싶어\n",
    "Ra-ta-ta-ta 울린 심장 (Ra-ta-ta-ta)\n",
    "I got nothing to lose\n",
    "널 좋아한다고\n",
    "wooah wooah wooah\n",
    "Ra-ta-ta-ta 울린 심장 (Ra-ta-ta-ta)\n",
    "But I don't want to\n",
    "Stay in the middle\n",
    "Like you a little\n",
    "Don’t want no riddle\n",
    "말해줘 say it back\n",
    "Oh say it ditto\n",
    "아침은 너무 멀어\n",
    "So say it ditto\n",
    "I don't want to\n",
    "Walk in this 미로\n",
    "다 아는 건 아니어도\n",
    "바라던 대로\n",
    "말해줘 Say it back\n",
    "Oh say it ditto\n",
    "I want you so, want you\n",
    "So say it ditto\n",
    "Not just anybody\n",
    "너를 상상했지\n",
    "항상 닿아있던\n",
    "처음 느낌 그대로 난\n",
    "기다렸지 all this time\n",
    "I got nothing to lose\n",
    "널 좋아한다고\n",
    "wooah wooah wooah\n",
    "Ra-ta-ta-ta 울린 심장 (Ra-ta-ta-ta)\n",
    "But I don't want to\n",
    "Stay in the middle\n",
    "Like you a little\n",
    "Don’t want no riddle\n",
    "말해줘 say it back\n",
    "Oh say it ditto\n",
    "아침은 너무 멀어\n",
    "So say it ditto\n",
    "I don't want to\n",
    "Walk in this 미로\n",
    "다 아는 건 아니어도\n",
    "바라던 대로\n",
    "말해줘 Say it back\n",
    "Oh say it ditto\n",
    "I want you so, want you\n",
    "So say it ditto\n",
    "Woo woo woo woo ooh\n",
    "Woo woo woo woo'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06ce6645-83fd-46f8-9cb9-54c791474e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7808d850-85d1-4ae7-bbb4-f4ff0423002d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "■ ■ ■ 원문 ■ ■ ■\n",
      " ['woo', 'woo', 'woo', 'woo', 'ooh', '\\n', 'woo', 'woo', 'woo', 'woo', '\\n', 'stay', 'in', 'the', 'middle', '\\n', 'like', 'you', 'a', 'little', '\\n', 'don', '’', 't', 'want', 'no', 'riddle', '\\n', '말', '해줘', 'say', 'it', 'back', '\\n', 'oh', 'say', 'it', 'ditto', '\\n', '아침', '은', '너무', '멀어', '\\n', 'so', 'say', 'it', 'ditto', '\\n', '훌쩍', '커', '버렸어', '\\n', '함께한', '기억', '처럼', '\\n', '널', '보는', '내', '마음', '은', '\\n', '어느새', '여름', '지나', '가을', '\\n', '기다렸지', 'all', 'this', 'time', '\\n', 'do', 'you', 'want', 'somebody', '\\n', 'like', 'i', 'want', 'somebody', '\\n', '날', '보고', '웃었지만', '\\n', 'do', 'you', 'think', 'about', 'me', 'now', 'yeah', '\\n', 'all', 'the', 'time', 'yeah', '\\n', 'all', 'the', 'time', '\\n', 'i', 'got', 'no', 'time', 'to', 'lose', '\\n', '내', '길었던', '하루', '\\n', '난', '보고', '싶어', '\\n', 'ra', '-', 'ta', '-', 'ta', '-', 'ta', '울린', '심장', '(', 'ra', '-', 'ta', '-', 'ta', '-', 'ta', ')', '\\n', 'i', 'got', 'nothing', 'to', 'lose', '\\n', '널', '좋아한다고', '\\n', 'wooah', 'wooah', 'wooah', '\\n', 'ra', '-', 'ta', '-', 'ta', '-', 'ta', '울린', '심장', '(', 'ra', '-', 'ta', '-', 'ta', '-', 'ta', ')', '\\n', 'but', 'i', 'don', \"'\", 't', 'want', 'to', '\\n', 'stay', 'in', 'the', 'middle', '\\n', 'like', 'you', 'a', 'little', '\\n', 'don', '’', 't', 'want', 'no', 'riddle', '\\n', '말', '해줘', 'say', 'it', 'back', '\\n', 'oh', 'say', 'it', 'ditto', '\\n', '아침', '은', '너무', '멀어', '\\n', 'so', 'say', 'it', 'ditto', '\\n', 'i', 'don', \"'\", 't', 'want', 'to', '\\n', 'walk', 'in', 'this', '미로', '\\n', '다', '아는', '건', '아니어도', '\\n', '바라던', '대로', '\\n', '말', '해줘', 'say', 'it', 'back', '\\n', 'oh', 'say', 'it', 'ditto', '\\n', 'i', 'want', 'you', 'so', ',', 'want', 'you', '\\n', 'so', 'say', 'it', 'ditto', '\\n', 'not', 'just', 'anybody', '\\n', '너', '를', '상상', '했지', '\\n', '항상', '닿아있던', '\\n', '처음', '느낌', '그대로', '난', '\\n', '기다렸지', 'all', 'this', 'time', '\\n', 'i', 'got', 'nothing', 'to', 'lose', '\\n', '널', '좋아한다고', '\\n', 'wooah', 'wooah', 'wooah', '\\n', 'ra', '-', 'ta', '-', 'ta', '-', 'ta', '울린', '심장', '(', 'ra', '-', 'ta', '-', 'ta', '-', 'ta', ')', '\\n', 'but', 'i', 'don', \"'\", 't', 'want', 'to', '\\n', 'stay', 'in', 'the', 'middle', '\\n', 'like', 'you', 'a', 'little', '\\n', 'don', '’', 't', 'want', 'no', 'riddle', '\\n', '말', '해줘', 'say', 'it', 'back', '\\n', 'oh', 'say', 'it', 'ditto', '\\n', '아침', '은', '너무', '멀어', '\\n', 'so', 'say', 'it', 'ditto', '\\n', 'i', 'don', \"'\", 't', 'want', 'to', '\\n', 'walk', 'in', 'this', '미로', '\\n', '다', '아는', '건', '아니어도', '\\n', '바라던', '대로', '\\n', '말', '해줘', 'say', 'it', 'back', '\\n', 'oh', 'say', 'it', 'ditto', '\\n', 'i', 'want', 'you', 'so', ',', 'want', 'you', '\\n', 'so', 'say', 'it', 'ditto', '\\n', 'woo', 'woo', 'woo', 'woo', 'ooh', '\\n', 'woo', 'woo', 'woo', 'woo']\n",
      "■ ■ ■ 1차 ■ ■ ■\n",
      " ['woo', 'woo', 'woo', 'woo', 'ooh', 'woo', 'woo', 'woo', 'woo', 'stay', 'in', 'the', 'middle', 'like', 'you', 'little', 'don', 'want', 'no', 'riddle', '해줘', 'say', 'it', 'back', 'oh', 'say', 'it', 'ditto', '아침', '너무', '멀어', 'so', 'say', 'it', 'ditto', '훌쩍', '버렸어', '함께한', '기억', '처럼', '보는', '마음', '어느새', '여름', '지나', '가을', '기다렸지', 'all', 'this', 'time', 'do', 'you', 'want', 'somebody', 'like', 'want', 'somebody', '보고', '웃었지만', 'do', 'you', 'think', 'about', 'me', 'now', 'yeah', 'all', 'the', 'time', 'yeah', 'all', 'the', 'time', 'got', 'no', 'time', 'to', 'lose', '길었던', '하루', '보고', '싶어', 'ra', 'ta', 'ta', 'ta', '울린', '심장', 'ra', 'ta', 'ta', 'ta', 'got', 'nothing', 'to', 'lose', '좋아한다고', 'wooah', 'wooah', 'wooah', 'ra', 'ta', 'ta', 'ta', '울린', '심장', 'ra', 'ta', 'ta', 'ta', 'but', 'don', 'want', 'to', 'stay', 'in', 'the', 'middle', 'like', 'you', 'little', 'don', 'want', 'no', 'riddle', '해줘', 'say', 'it', 'back', 'oh', 'say', 'it', 'ditto', '아침', '너무', '멀어', 'so', 'say', 'it', 'ditto', 'don', 'want', 'to', 'walk', 'in', 'this', '미로', '아는', '아니어도', '바라던', '대로', '해줘', 'say', 'it', 'back', 'oh', 'say', 'it', 'ditto', 'want', 'you', 'so', 'want', 'you', 'so', 'say', 'it', 'ditto', 'not', 'just', 'anybody', '상상', '했지', '항상', '닿아있던', '처음', '느낌', '그대로', '기다렸지', 'all', 'this', 'time', 'got', 'nothing', 'to', 'lose', '좋아한다고', 'wooah', 'wooah', 'wooah', 'ra', 'ta', 'ta', 'ta', '울린', '심장', 'ra', 'ta', 'ta', 'ta', 'but', 'don', 'want', 'to', 'stay', 'in', 'the', 'middle', 'like', 'you', 'little', 'don', 'want', 'no', 'riddle', '해줘', 'say', 'it', 'back', 'oh', 'say', 'it', 'ditto', '아침', '너무', '멀어', 'so', 'say', 'it', 'ditto', 'don', 'want', 'to', 'walk', 'in', 'this', '미로', '아는', '아니어도', '바라던', '대로', '해줘', 'say', 'it', 'back', 'oh', 'say', 'it', 'ditto', 'want', 'you', 'so', 'want', 'you', 'so', 'say', 'it', 'ditto', 'woo', 'woo', 'woo', 'woo', 'ooh', 'woo', 'woo', 'woo', 'woo']\n",
      "■ ■ ■ 2차 ■ ■ ■\n",
      " ['woo', 'woo', 'woo', 'woo', 'ooh', 'woo', 'woo', 'woo', 'woo', 'stay', 'in', 'the', 'middle', 'like', 'you', 'little', 'don', 'want', 'no', 'riddle', '해줘', 'say', 'it', 'back', 'oh', 'say', 'it', 'ditto', '아침', '너무', '멀어', 'so', 'say', 'it', 'ditto', '훌쩍', '버렸어', '함께한', '기억', '처럼', '보는', '마음', '어느새', '여름', '지나', '가을', '기다렸지', 'all', 'this', 'time', 'do', 'you', 'want', 'somebody', 'like', 'want', 'somebody', '보고', '웃었지만', 'do', 'you', 'think', 'about', 'me', 'now', 'yeah', 'all', 'the', 'time', 'yeah', 'all', 'the', 'time', 'got', 'no', 'time', 'to', 'lose', '길었던', '하루', '보고', '싶어', 'ra', 'ta', 'ta', 'ta', '울린', '심장', 'ra', 'ta', 'ta', 'ta', 'got', 'nothing', 'to', 'lose', '좋아한다고', 'wooah', 'wooah', 'wooah', 'ra', 'ta', 'ta', 'ta', '울린', '심장', 'ra', 'ta', 'ta', 'ta', 'but', 'don', 'want', 'to', 'stay', 'in', 'the', 'middle', 'like', 'you', 'little', 'don', 'want', 'no', 'riddle', '해줘', 'say', 'it', 'back', 'oh', 'say', 'it', 'ditto', '아침', '너무', '멀어', 'so', 'say', 'it', 'ditto', 'don', 'want', 'to', 'walk', 'in', 'this', '미로', '아는', '아니어도', '바라던', '대로', '해줘', 'say', 'it', 'back', 'oh', 'say', 'it', 'ditto', 'want', 'you', 'so', 'want', 'you', 'so', 'say', 'it', 'ditto', 'not', 'just', 'anybody', '상상', '했지', '항상', '닿아있던', '처음', '느낌', '그대로', '기다렸지', 'all', 'this', 'time', 'got', 'nothing', 'to', 'lose', '좋아한다고', 'wooah', 'wooah', 'wooah', 'ra', 'ta', 'ta', 'ta', '울린', '심장', 'ra', 'ta', 'ta', 'ta', 'but', 'don', 'want', 'to', 'stay', 'in', 'the', 'middle', 'like', 'you', 'little', 'don', 'want', 'no', 'riddle', '해줘', 'say', 'it', 'back', 'oh', 'say', 'it', 'ditto', '아침', '너무', '멀어', 'so', 'say', 'it', 'ditto', 'don', 'want', 'to', 'walk', 'in', 'this', '미로', '아는', '아니어도', '바라던', '대로', '해줘', 'say', 'it', 'back', 'oh', 'say', 'it', 'ditto', 'want', 'you', 'so', 'want', 'you', 'so', 'say', 'it', 'ditto', 'woo', 'woo', 'woo', 'woo', 'ooh', 'woo', 'woo', 'woo', 'woo']\n"
     ]
    }
   ],
   "source": [
    "# 정제 작업 : 선생님 방법 (한글 노래 가사여서 len으로 해도 자연스럽게 됨)\n",
    "token_ori = okt.morphs(text.lower())\n",
    "token_1 = [x for x in token_ori if len(x) > 1]\n",
    "token_2 = [x for x in token_1 if x not in ['\\n']]\n",
    "\n",
    "print(\"■ ■ ■ 원문 ■ ■ ■\\n\", token_ori)\n",
    "print(\"■ ■ ■ 1차 ■ ■ ■\\n\", token_1)\n",
    "print(\"■ ■ ■ 2차 ■ ■ ■\\n\", token_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a16d30d5-c2ea-44a5-a85b-324bd88e1042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "■ ■ ■ 원문 ■ ■ ■\n",
      " ['woo', 'woo', 'woo', 'woo', 'ooh', '\\n', 'woo', 'woo', 'woo', 'woo', '\\n', 'stay', 'in', 'the', 'middle', '\\n', 'like', 'you', 'a', 'little', '\\n', 'don', '’', 't', 'want', 'no', 'riddle', '\\n', '말', '해줘', 'say', 'it', 'back', '\\n', 'oh', 'say', 'it', 'ditto', '\\n', '아침', '은', '너무', '멀어', '\\n', 'so', 'say', 'it', 'ditto', '\\n', '훌쩍', '커', '버렸어', '\\n', '함께한', '기억', '처럼', '\\n', '널', '보는', '내', '마음', '은', '\\n', '어느새', '여름', '지나', '가을', '\\n', '기다렸지', 'all', 'this', 'time', '\\n', 'do', 'you', 'want', 'somebody', '\\n', 'like', 'i', 'want', 'somebody', '\\n', '날', '보고', '웃었지만', '\\n', 'do', 'you', 'think', 'about', 'me', 'now', 'yeah', '\\n', 'all', 'the', 'time', 'yeah', '\\n', 'all', 'the', 'time', '\\n', 'i', 'got', 'no', 'time', 'to', 'lose', '\\n', '내', '길었던', '하루', '\\n', '난', '보고', '싶어', '\\n', 'ra', '-', 'ta', '-', 'ta', '-', 'ta', '울린', '심장', '(', 'ra', '-', 'ta', '-', 'ta', '-', 'ta', ')', '\\n', 'i', 'got', 'nothing', 'to', 'lose', '\\n', '널', '좋아한다고', '\\n', 'wooah', 'wooah', 'wooah', '\\n', 'ra', '-', 'ta', '-', 'ta', '-', 'ta', '울린', '심장', '(', 'ra', '-', 'ta', '-', 'ta', '-', 'ta', ')', '\\n', 'but', 'i', 'don', \"'\", 't', 'want', 'to', '\\n', 'stay', 'in', 'the', 'middle', '\\n', 'like', 'you', 'a', 'little', '\\n', 'don', '’', 't', 'want', 'no', 'riddle', '\\n', '말', '해줘', 'say', 'it', 'back', '\\n', 'oh', 'say', 'it', 'ditto', '\\n', '아침', '은', '너무', '멀어', '\\n', 'so', 'say', 'it', 'ditto', '\\n', 'i', 'don', \"'\", 't', 'want', 'to', '\\n', 'walk', 'in', 'this', '미로', '\\n', '다', '아는', '건', '아니어도', '\\n', '바라던', '대로', '\\n', '말', '해줘', 'say', 'it', 'back', '\\n', 'oh', 'say', 'it', 'ditto', '\\n', 'i', 'want', 'you', 'so', ',', 'want', 'you', '\\n', 'so', 'say', 'it', 'ditto', '\\n', 'not', 'just', 'anybody', '\\n', '너', '를', '상상', '했지', '\\n', '항상', '닿아있던', '\\n', '처음', '느낌', '그대로', '난', '\\n', '기다렸지', 'all', 'this', 'time', '\\n', 'i', 'got', 'nothing', 'to', 'lose', '\\n', '널', '좋아한다고', '\\n', 'wooah', 'wooah', 'wooah', '\\n', 'ra', '-', 'ta', '-', 'ta', '-', 'ta', '울린', '심장', '(', 'ra', '-', 'ta', '-', 'ta', '-', 'ta', ')', '\\n', 'but', 'i', 'don', \"'\", 't', 'want', 'to', '\\n', 'stay', 'in', 'the', 'middle', '\\n', 'like', 'you', 'a', 'little', '\\n', 'don', '’', 't', 'want', 'no', 'riddle', '\\n', '말', '해줘', 'say', 'it', 'back', '\\n', 'oh', 'say', 'it', 'ditto', '\\n', '아침', '은', '너무', '멀어', '\\n', 'so', 'say', 'it', 'ditto', '\\n', 'i', 'don', \"'\", 't', 'want', 'to', '\\n', 'walk', 'in', 'this', '미로', '\\n', '다', '아는', '건', '아니어도', '\\n', '바라던', '대로', '\\n', '말', '해줘', 'say', 'it', 'back', '\\n', 'oh', 'say', 'it', 'ditto', '\\n', 'i', 'want', 'you', 'so', ',', 'want', 'you', '\\n', 'so', 'say', 'it', 'ditto', '\\n', 'woo', 'woo', 'woo', 'woo', 'ooh', '\\n', 'woo', 'woo', 'woo', 'woo']\n",
      "■ ■ ■ 1차 ■ ■ ■\n",
      " ['stay', 'middle', 'like', 'little', 'want', 'riddle', '해줘', 'back', 'ditto', '아침', '너무', '멀어', 'ditto', '훌쩍', '버렸어', '함께한', '기억', '처럼', '보는', '마음', '어느새', '여름', '지나', '가을', '기다렸지', 'this', 'time', 'want', 'somebody', 'like', 'want', 'somebody', '보고', '웃었지만', 'think', 'about', 'yeah', 'time', 'yeah', 'time', 'time', 'lose', '길었던', '하루', '보고', '싶어', '울린', '심장', 'nothing', 'lose', '좋아한다고', 'wooah', 'wooah', 'wooah', '울린', '심장', 'want', 'stay', 'middle', 'like', 'little', 'want', 'riddle', '해줘', 'back', 'ditto', '아침', '너무', '멀어', 'ditto', 'want', 'walk', 'this', '미로', '아는', '아니어도', '바라던', '대로', '해줘', 'back', 'ditto', 'want', 'want', 'ditto', 'just', 'anybody', '상상', '했지', '항상', '닿아있던', '처음', '느낌', '그대로', '기다렸지', 'this', 'time', 'nothing', 'lose', '좋아한다고', 'wooah', 'wooah', 'wooah', '울린', '심장', 'want', 'stay', 'middle', 'like', 'little', 'want', 'riddle', '해줘', 'back', 'ditto', '아침', '너무', '멀어', 'ditto', 'want', 'walk', 'this', '미로', '아는', '아니어도', '바라던', '대로', '해줘', 'back', 'ditto', 'want', 'want', 'ditto']\n",
      "■ ■ ■ 2차 ■ ■ ■\n",
      " ['stay', 'middle', 'like', 'little', 'want', 'riddle', '해줘', 'back', 'ditto', '아침', '너무', '멀어', 'ditto', '훌쩍', '버렸어', '함께한', '기억', '처럼', '보는', '마음', '어느새', '여름', '지나', '가을', '기다렸지', 'this', 'time', 'want', 'somebody', 'like', 'want', 'somebody', '보고', '웃었지만', 'think', 'about', 'yeah', 'time', 'yeah', 'time', 'time', 'lose', '길었던', '하루', '보고', '싶어', '울린', '심장', 'nothing', 'lose', '좋아한다고', 'wooah', 'wooah', 'wooah', '울린', '심장', 'want', 'stay', 'middle', 'like', 'little', 'want', 'riddle', '해줘', 'back', 'ditto', '아침', '너무', '멀어', 'ditto', 'want', 'walk', 'this', '미로', '아는', '아니어도', '바라던', '대로', '해줘', 'back', 'ditto', 'want', 'want', 'ditto', 'just', 'anybody', '상상', '했지', '항상', '닿아있던', '처음', '느낌', '그대로', '기다렸지', 'this', 'time', 'nothing', 'lose', '좋아한다고', 'wooah', 'wooah', 'wooah', '울린', '심장', 'want', 'stay', 'middle', 'like', 'little', 'want', 'riddle', '해줘', 'back', 'ditto', '아침', '너무', '멀어', 'ditto', 'want', 'walk', 'this', '미로', '아는', '아니어도', '바라던', '대로', '해줘', 'back', 'ditto', 'want', 'want', 'ditto']\n"
     ]
    }
   ],
   "source": [
    "# 정제 작업 : 내 방법 (영문이 섞여, len보다는 byte기준으로 단어를 자름)\n",
    "# byte 단위 자르기 방법 : https://onlytojay.medium.com/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%AC%B8%EC%9E%90%EA%B8%B8%EC%9D%B4-%EA%B7%B8%EB%A6%AC%EA%B3%A0-%EB%B0%94%EC%9D%B4%ED%8A%B8-6d555f1fe126\n",
    "token_ori = okt.morphs(text.lower())\n",
    "token_1 = [x for x in token_ori if len(x.encode()) > 3]\n",
    "token_2 = [x for x in token_1 if x not in ['\\n']]\n",
    "\n",
    "print(\"■ ■ ■ 원문 ■ ■ ■\\n\", token_ori)\n",
    "print(\"■ ■ ■ 1차 ■ ■ ■\\n\", token_1)\n",
    "print(\"■ ■ ■ 2차 ■ ■ ■\\n\", token_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69cdb724-a94a-4af2-b6b5-71f35b873944",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('want', 13),\n",
       " ('ditto', 10),\n",
       " ('wooah', 6),\n",
       " ('해줘', 5),\n",
       " ('back', 5),\n",
       " ('time', 5),\n",
       " ('like', 4),\n",
       " ('this', 4),\n",
       " ('stay', 3),\n",
       " ('middle', 3),\n",
       " ('little', 3),\n",
       " ('riddle', 3),\n",
       " ('아침', 3),\n",
       " ('너무', 3),\n",
       " ('멀어', 3),\n",
       " ('lose', 3),\n",
       " ('울린', 3),\n",
       " ('심장', 3),\n",
       " ('기다렸지', 2),\n",
       " ('somebody', 2),\n",
       " ('보고', 2),\n",
       " ('yeah', 2),\n",
       " ('nothing', 2),\n",
       " ('좋아한다고', 2),\n",
       " ('walk', 2),\n",
       " ('미로', 2),\n",
       " ('아는', 2),\n",
       " ('아니어도', 2),\n",
       " ('바라던', 2),\n",
       " ('대로', 2),\n",
       " ('훌쩍', 1),\n",
       " ('버렸어', 1),\n",
       " ('함께한', 1),\n",
       " ('기억', 1),\n",
       " ('처럼', 1),\n",
       " ('보는', 1),\n",
       " ('마음', 1),\n",
       " ('어느새', 1),\n",
       " ('여름', 1),\n",
       " ('지나', 1),\n",
       " ('가을', 1),\n",
       " ('웃었지만', 1),\n",
       " ('think', 1),\n",
       " ('about', 1),\n",
       " ('길었던', 1),\n",
       " ('하루', 1),\n",
       " ('싶어', 1),\n",
       " ('just', 1),\n",
       " ('anybody', 1),\n",
       " ('상상', 1),\n",
       " ('했지', 1),\n",
       " ('항상', 1),\n",
       " ('닿아있던', 1),\n",
       " ('처음', 1),\n",
       " ('느낌', 1),\n",
       " ('그대로', 1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어장과 표현 빈도 사전 만들기\n",
    "vocab = {}\n",
    "\n",
    "for word in token_2:\n",
    "    if word not in vocab:\n",
    "        vocab[word] = 1\n",
    "    else:\n",
    "        vocab[word] += 1\n",
    "\n",
    "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse=True)\n",
    "vocab_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eba61d24-5de4-4d3b-9f12-f6790f02f094",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'want': 1,\n",
       " 'ditto': 2,\n",
       " 'wooah': 3,\n",
       " '해줘': 4,\n",
       " 'back': 5,\n",
       " 'time': 6,\n",
       " 'like': 7,\n",
       " 'this': 8,\n",
       " 'stay': 9,\n",
       " 'middle': 10,\n",
       " 'little': 11,\n",
       " 'riddle': 12,\n",
       " '아침': 13,\n",
       " '너무': 14,\n",
       " '멀어': 15,\n",
       " 'lose': 16,\n",
       " '울린': 17,\n",
       " '심장': 18,\n",
       " '기다렸지': 19,\n",
       " 'somebody': 20,\n",
       " '보고': 21,\n",
       " 'yeah': 22,\n",
       " 'nothing': 23,\n",
       " '좋아한다고': 24,\n",
       " 'walk': 25,\n",
       " '미로': 26,\n",
       " '아는': 27,\n",
       " '아니어도': 28,\n",
       " '바라던': 29,\n",
       " '대로': 30}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인코딩 인덱스 리스트 만들기\n",
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, freq) in vocab_sorted:\n",
    "    if freq > 1:\n",
    "        i +=1\n",
    "        word_to_index[word] = i\n",
    "\n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4c7c304-d468-4904-92c4-ae19297babf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 주요 단어 추출 전 : \n",
      " {'want': 1, 'ditto': 2, 'wooah': 3, '해줘': 4, 'back': 5, 'time': 6, 'like': 7, 'this': 8, 'stay': 9, 'middle': 10, 'little': 11, 'riddle': 12, '아침': 13, '너무': 14, '멀어': 15, 'lose': 16, '울린': 17, '심장': 18, '기다렸지': 19, 'somebody': 20, '보고': 21, 'yeah': 22, 'nothing': 23, '좋아한다고': 24, 'walk': 25, '미로': 26, '아는': 27, '아니어도': 28, '바라던': 29, '대로': 30}\n",
      " 주요 단어 추출 후 : \n",
      " {'want': 1, 'ditto': 2, 'wooah': 3, '해줘': 4, 'back': 5}\n"
     ]
    }
   ],
   "source": [
    "# 주요 단어 추출 방법 1 : 선생님 방법\n",
    "# 주요 단어 5개만 선택한 후, 그 외 단어는 추출\n",
    "# 우선, 주요하지 않은 단어의 리스트를 담는다.\n",
    "\n",
    "print(\" 주요 단어 추출 전 : \\n\", word_to_index)\n",
    "\n",
    "vocab_size = 5\n",
    "word_freq_cut = [word for word, index in word_to_index.items() if index >= vocab_size + 1]\n",
    "word_freq_cut\n",
    "\n",
    "for w in word_freq_cut:\n",
    "    del word_to_index[w]\n",
    "    \n",
    "print(\" 주요 단어 추출 후 : \\n\", word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ea124ef-9d0a-4180-8e6f-238de812e088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'want': 1, 'ditto': 2, 'wooah': 3, '해줘': 4, 'back': 5, 'OOV': 6}\n"
     ]
    }
   ],
   "source": [
    "# 예외사항(기타) 만들기\n",
    "word_to_index[\"OOV\"] = len(word_to_index) + 1\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2875843-552a-458d-a1e0-80509f82456d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'want': 1, 'ditto': 2, 'wooah': 3, '해줘': 4, 'back': 5, 'OOV': 6}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 주요 단어 추출 방법 2 : 직접 해봄\n",
    "# 나는 아래와 같이 바꾸어봤다. 두 단계를 한 단계로 했다는 점이 좋긴 하지만.. 잘리는 단어가 무언지 볼 수가 없다는 단점.\n",
    "# Dictionary Comprehension을 이용했으니 이 부분 특기해야 함!\n",
    "# 참고 : http://pythonstudy.xyz/python/article/22-Python-Comprehension\n",
    "\n",
    "vocab_size = 5\n",
    "word_important = [word for word, index in word_to_index.items() if index <= vocab_size]\n",
    "word_to_index = {key : index+1 for index, key in enumerate(word_important)}\n",
    "word_to_index[\"OOV\"] = len(word_to_index) + 1\n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d15f59a0-5e36-4868-adc4-d8b28fe4f034",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가사를 인코딩 숫자로 변경하여 보기\n",
    "encoded_sentences = []\n",
    "for word in token_1:\n",
    "    try:\n",
    "        encoded_sentences.append(word_to_index[word])\n",
    "    except:\n",
    "        encoded_sentences.append(word_to_index['OOV'])\n",
    "\n",
    "encoded_sentences        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9d55f523-c72d-42c0-99cf-2d3eea102fa0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'woo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 인코딩된 숫자와 가사 동시에 보기 (구현 필요부)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m lyrics \u001b[38;5;241m=\u001b[39m [ (token, word_to_index[token]) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m token_ori]\n",
      "Cell \u001b[1;32mIn[88], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 인코딩된 숫자와 가사 동시에 보기 (구현 필요부)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m lyrics \u001b[38;5;241m=\u001b[39m [ (token, \u001b[43mword_to_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m token_ori]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'woo'"
     ]
    }
   ],
   "source": [
    "# 인코딩된 숫자와 가사 동시에 보기 (구현 필요부)\n",
    "# woo와 같은 기타 사항까지 하려면 try except를 적용해야 한다.\n",
    "lyrics = [ (token, word_to_index[token]) for token in token_ori]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f575bdd-ebdf-4b1c-8c5b-61db4ec38d35",
   "metadata": {},
   "source": [
    "## 가사 인코딩해보기 (라이브러리 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576e368-d352-412d-9c5a-db686c113dd6",
   "metadata": {},
   "source": [
    "* keras.preprocessing.text 의 Tokenizer 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d403c70a-bc5c-476c-843a-7e43f2779881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6146e91f-20fd-4e3c-a863-cc8ba1f78e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Woo woo woo woo ooh\n",
    "Woo woo woo woo\n",
    "Stay in the middle\n",
    "Like you a little\n",
    "Don’t want no riddle\n",
    "말해줘 say it back\n",
    "Oh say it ditto\n",
    "아침은 너무 멀어\n",
    "So say it ditto\n",
    "훌쩍 커버렸어\n",
    "함께한 기억처럼\n",
    "널 보는 내 마음은\n",
    "어느새 여름 지나 가을\n",
    "기다렸지 all this time\n",
    "Do you want somebody\n",
    "Like I want somebody\n",
    "날 보고 웃었지만\n",
    "Do you think about me now yeah\n",
    "All the time yeah\n",
    "All the time\n",
    "I got no time to lose\n",
    "내 길었던 하루\n",
    "난 보고 싶어\n",
    "Ra-ta-ta-ta 울린 심장 (Ra-ta-ta-ta)\n",
    "I got nothing to lose\n",
    "널 좋아한다고\n",
    "wooah wooah wooah\n",
    "Ra-ta-ta-ta 울린 심장 (Ra-ta-ta-ta)\n",
    "But I don't want to\n",
    "Stay in the middle\n",
    "Like you a little\n",
    "Don’t want no riddle\n",
    "말해줘 say it back\n",
    "Oh say it ditto\n",
    "아침은 너무 멀어\n",
    "So say it ditto\n",
    "I don't want to\n",
    "Walk in this 미로\n",
    "다 아는 건 아니어도\n",
    "바라던 대로\n",
    "말해줘 Say it back\n",
    "Oh say it ditto\n",
    "I want you so, want you\n",
    "So say it ditto\n",
    "Not just anybody\n",
    "너를 상상했지\n",
    "항상 닿아있던\n",
    "처음 느낌 그대로 난\n",
    "기다렸지 all this time\n",
    "I got nothing to lose\n",
    "널 좋아한다고\n",
    "wooah wooah wooah\n",
    "Ra-ta-ta-ta 울린 심장 (Ra-ta-ta-ta)\n",
    "But I don't want to\n",
    "Stay in the middle\n",
    "Like you a little\n",
    "Don’t want no riddle\n",
    "말해줘 say it back\n",
    "Oh say it ditto\n",
    "아침은 너무 멀어\n",
    "So say it ditto\n",
    "I don't want to\n",
    "Walk in this 미로\n",
    "다 아는 건 아니어도\n",
    "바라던 대로\n",
    "말해줘 Say it back\n",
    "Oh say it ditto\n",
    "I want you so, want you\n",
    "So say it ditto\n",
    "Woo woo woo woo ooh\n",
    "Woo woo woo woo'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "19e38ac4-435c-4d22-a4f1-ffeea7a6cc2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = text.lower().split('\\n')\n",
    "text = [x.split(' ') for x in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b3920b0e-eba3-46c2-a78e-afd7be463727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 정의\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# 토크나이저가 분리할 텍스트 fit\n",
    "tokenizer.fit_on_texts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "91c2420d-0aec-404d-96db-9ef4e27b2fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "■■■ index : \n",
      " {'woo': 1, 'say': 2, 'it': 3, 'want': 4, 'ditto': 5, 'i': 6, 'you': 7, 'to': 8, 'wooah': 9, 'in': 10, 'the': 11, '말해줘': 12, 'back': 13, 'oh': 14, 'so': 15, 'time': 16, 'like': 17, 'no': 18, 'all': 19, 'this': 20, \"don't\": 21, 'stay': 22, 'middle': 23, 'a': 24, 'little': 25, 'don’t': 26, 'riddle': 27, '아침은': 28, '너무': 29, '멀어': 30, '널': 31, 'got': 32, 'lose': 33, 'ra-ta-ta-ta': 34, '울린': 35, '심장': 36, '(ra-ta-ta-ta)': 37, 'ooh': 38, '내': 39, '기다렸지': 40, 'do': 41, 'somebody': 42, '보고': 43, 'yeah': 44, '난': 45, 'nothing': 46, '좋아한다고': 47, 'but': 48, 'walk': 49, '미로': 50, '다': 51, '아는': 52, '건': 53, '아니어도': 54, '바라던': 55, '대로': 56, 'so,': 57, '훌쩍': 58, '커버렸어': 59, '함께한': 60, '기억처럼': 61, '보는': 62, '마음은': 63, '어느새': 64, '여름': 65, '지나': 66, '가을': 67, '날': 68, '웃었지만': 69, 'think': 70, 'about': 71, 'me': 72, 'now': 73, '길었던': 74, '하루': 75, '싶어': 76, 'not': 77, 'just': 78, 'anybody': 79, '너를': 80, '상상했지': 81, '항상': 82, '닿아있던': 83, '처음': 84, '느낌': 85, '그대로': 86}\n",
      "■■■ count : \n",
      " OrderedDict([('woo', 16), ('ooh', 2), ('stay', 3), ('in', 5), ('the', 5), ('middle', 3), ('like', 4), ('you', 9), ('a', 3), ('little', 3), ('don’t', 3), ('want', 13), ('no', 4), ('riddle', 3), ('말해줘', 5), ('say', 15), ('it', 15), ('back', 5), ('oh', 5), ('ditto', 10), ('아침은', 3), ('너무', 3), ('멀어', 3), ('so', 5), ('훌쩍', 1), ('커버렸어', 1), ('함께한', 1), ('기억처럼', 1), ('널', 3), ('보는', 1), ('내', 2), ('마음은', 1), ('어느새', 1), ('여름', 1), ('지나', 1), ('가을', 1), ('기다렸지', 2), ('all', 4), ('this', 4), ('time', 5), ('do', 2), ('somebody', 2), ('i', 10), ('날', 1), ('보고', 2), ('웃었지만', 1), ('think', 1), ('about', 1), ('me', 1), ('now', 1), ('yeah', 2), ('got', 3), ('to', 7), ('lose', 3), ('길었던', 1), ('하루', 1), ('난', 2), ('싶어', 1), ('ra-ta-ta-ta', 3), ('울린', 3), ('심장', 3), ('(ra-ta-ta-ta)', 3), ('nothing', 2), ('좋아한다고', 2), ('wooah', 6), ('but', 2), (\"don't\", 4), ('walk', 2), ('미로', 2), ('다', 2), ('아는', 2), ('건', 2), ('아니어도', 2), ('바라던', 2), ('대로', 2), ('so,', 2), ('not', 1), ('just', 1), ('anybody', 1), ('너를', 1), ('상상했지', 1), ('항상', 1), ('닿아있던', 1), ('처음', 1), ('느낌', 1), ('그대로', 1)])\n"
     ]
    }
   ],
   "source": [
    "# 텍스트에서 등장하는 단어에 대한 index 확인\n",
    "print(\"■■■ index : \\n\", tokenizer.word_index)\n",
    "\n",
    "# 텍스트들 등장 횟수 확인\n",
    "print(\"■■■ count : \\n\", tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5ae3fb21-1ef6-4105-8be9-ae0d1d4d7abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "■■■ word index  :  {'woo': 1, 'say': 2, 'it': 3, 'want': 4, 'ditto': 5}\n",
      "■■■ word counts :  OrderedDict([('woo', 16), ('want', 13), ('say', 15), ('it', 15), ('ditto', 10)])\n",
      "■■■ sentences :  [[1, 1, 1, 1], [1, 1, 1, 1], [], [], [4], [2, 3], [2, 3, 5], [], [2, 3, 5], [], [], [], [], [], [4], [4], [], [], [], [], [], [], [], [], [], [], [], [], [4], [], [], [4], [2, 3], [2, 3, 5], [], [2, 3, 5], [4], [], [], [], [2, 3], [2, 3, 5], [4, 4], [2, 3, 5], [], [], [], [], [], [], [], [], [], [4], [], [], [4], [2, 3], [2, 3, 5], [], [2, 3, 5], [4], [], [], [], [2, 3], [2, 3, 5], [4, 4], [2, 3, 5], [1, 1, 1, 1], [1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# 가사 전체 tokenize 하기 (1)\n",
    "\n",
    "## 주요 단어 개수 지정\n",
    "vocab_size = 5\n",
    "tok = Tokenizer(num_words = vocab_size + 1)\n",
    "tok.fit_on_texts(text)\n",
    "\n",
    "## 비주요 단어 추출\n",
    "words_freq_cut = [word for word, index in tok.word_index.items()\n",
    "             if index >= vocab_size +1]\n",
    "words_freq_cut\n",
    "\n",
    "## 주요 단어만 남기기\n",
    "for word in words_freq_cut:\n",
    "    del tok.word_index[word]\n",
    "    del tok.word_counts[word]\n",
    "\n",
    "## 정제 후 결과 출력해보기\n",
    "print(\"■■■ word index  : \", tok.word_index)\n",
    "print(\"■■■ word counts : \", tok.word_counts)\n",
    "print(\"■■■ sentences : \", tok.texts_to_sequences(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7eb95fdc-c799-434d-a781-9ba14a538e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "■■■ word index  : \n",
      " {'oov': 1, 'woo': 2, 'say': 3}\n",
      "■■■ word counts : \n",
      " OrderedDict([('woo', 16), ('say', 15)])\n",
      "■■■ sentences : \n",
      " [[2, 2, 2, 2, 1], [2, 2, 2, 2], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 3, 1, 1], [1, 3, 1, 1], [1, 1, 1], [1, 3, 1, 1], [1, 1], [1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 3, 1, 1], [1, 3, 1, 1], [1, 1, 1], [1, 3, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1], [1, 3, 1, 1], [1, 3, 1, 1], [1, 1, 1, 1, 1, 1], [1, 3, 1, 1], [1, 1, 1], [1, 1], [1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 3, 1, 1], [1, 3, 1, 1], [1, 1, 1], [1, 3, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1], [1, 3, 1, 1], [1, 3, 1, 1], [1, 1, 1, 1, 1, 1], [1, 3, 1, 1], [2, 2, 2, 2, 1], [2, 2, 2, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 가사 전체 tokenize 하기 (2) + padding\n",
    "\n",
    "## 주요 단어 개수 지정\n",
    "vocab_size = 3\n",
    "tok_oov = Tokenizer(num_words = vocab_size +2, oov_token='oov') # 0과 oov(out of vocabulary)를 넣기 위해 2개를 더 넣음\n",
    "tok_oov.fit_on_texts(text)\n",
    "\n",
    "## oov(out of vocabulary) 추가\n",
    "## 비주요 단어 추출\n",
    "words_freq_cut = [word for word, index in tok_oov.word_index.items()\n",
    "             if index >= vocab_size +1]\n",
    "words_freq_cut\n",
    "\n",
    "## 주요 단어만 남기기\n",
    "for word in words_freq_cut:\n",
    "    del tok_oov.word_index[word]\n",
    "    del tok_oov.word_counts[word]\n",
    "    \n",
    "## 정제 후 결과 출력해보기\n",
    "print(\"■■■ word index  : \\n\", tok_oov.word_index)\n",
    "print(\"■■■ word counts : \\n\", tok_oov.word_counts)\n",
    "print(\"■■■ sentences : \\n\", tok_oov.texts_to_sequences(text))\n",
    "encoded_text = tok_oov.texts_to_sequences(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff7a125-99f9-4ece-9c8a-808b555b3565",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232213b0-01c4-4973-aedb-d36b3be1f7dc",
   "metadata": {},
   "source": [
    "* 자연어 처리에서의 padding\n",
    "* 문장의 길이가 짧은 문장에 대해 나머지 부분을 0으로 채워주기 위함  \n",
    "* 이는, 인공신경망을 통한 분석을 위한 작업 (크기 맞춰주기)이 된다.\n",
    "https://velog.io/@ganta/%ED%8C%A8%EB%94%A9Padding  \n",
    "* 0이 앞쪽에 채워지는 것이 기본이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199f6e4a-0f36-482b-b77e-fcc97eba78d7",
   "metadata": {},
   "source": [
    "## 하드 코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "dcd06641-0a8d-4afd-9a4b-3faa6e97d282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최대 단어 수를 지닌 문장의 최대 단어 수 확인\n",
    "MAX_LEN = max([len(x) for x in encoded_text])\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c8d37a33-a7cf-4cb0-8af8-9430f700192c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 2, 2, 2, 1, 0, 0],\n",
       " [2, 2, 2, 2, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 3, 1, 1, 0, 0, 0],\n",
       " [1, 3, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 0, 0, 0, 0],\n",
       " [1, 3, 1, 1, 0, 0, 0],\n",
       " [1, 1, 0, 0, 0, 0, 0],\n",
       " [1, 1, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 0],\n",
       " [1, 1, 1, 0, 0, 0, 0],\n",
       " [1, 1, 1, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 0, 0],\n",
       " [1, 1, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 3, 1, 1, 0, 0, 0],\n",
       " [1, 3, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 0, 0, 0, 0],\n",
       " [1, 3, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 0, 0, 0, 0, 0],\n",
       " [1, 3, 1, 1, 0, 0, 0],\n",
       " [1, 3, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 0],\n",
       " [1, 3, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 0, 0, 0, 0],\n",
       " [1, 1, 0, 0, 0, 0, 0],\n",
       " [1, 1, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 0, 0],\n",
       " [1, 1, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 3, 1, 1, 0, 0, 0],\n",
       " [1, 3, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 0, 0, 0, 0],\n",
       " [1, 3, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 0, 0],\n",
       " [1, 1, 0, 0, 0, 0, 0],\n",
       " [1, 3, 1, 1, 0, 0, 0],\n",
       " [1, 3, 1, 1, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 0],\n",
       " [1, 3, 1, 1, 0, 0, 0],\n",
       " [2, 2, 2, 2, 1, 0, 0],\n",
       " [2, 2, 2, 2, 0, 0, 0]]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sentence in encoded_text:\n",
    "    while len(sentence) < MAX_LEN:\n",
    "        sentence.append(0)\n",
    "\n",
    "encoded_text\n",
    "# 하지만 이렇게 하면.. 0이 뒷쪽에서부터 채워지게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2794655d-9be4-4871-92e5-ed3d8c23f8b5",
   "metadata": {},
   "source": [
    "## 라이브러리 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "144dd562-b548-4c88-ae90-74862e4022d1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 38],\n",
       " [1, 1, 1, 1],\n",
       " [22, 10, 11, 23],\n",
       " [17, 7, 24, 25],\n",
       " [26, 4, 18, 27],\n",
       " [12, 2, 3, 13],\n",
       " [14, 2, 3, 5],\n",
       " [28, 29, 30],\n",
       " [15, 2, 3, 5],\n",
       " [58, 59],\n",
       " [60, 61],\n",
       " [31, 62, 39, 63],\n",
       " [64, 65, 66, 67],\n",
       " [40, 19, 20, 16],\n",
       " [41, 7, 4, 42],\n",
       " [17, 6, 4, 42],\n",
       " [68, 43, 69],\n",
       " [41, 7, 70, 71, 72, 73, 44],\n",
       " [19, 11, 16, 44],\n",
       " [19, 11, 16],\n",
       " [6, 32, 18, 16, 8, 33],\n",
       " [39, 74, 75],\n",
       " [45, 43, 76],\n",
       " [34, 35, 36, 37],\n",
       " [6, 32, 46, 8, 33],\n",
       " [31, 47],\n",
       " [9, 9, 9],\n",
       " [34, 35, 36, 37],\n",
       " [48, 6, 21, 4, 8],\n",
       " [22, 10, 11, 23],\n",
       " [17, 7, 24, 25],\n",
       " [26, 4, 18, 27],\n",
       " [12, 2, 3, 13],\n",
       " [14, 2, 3, 5],\n",
       " [28, 29, 30],\n",
       " [15, 2, 3, 5],\n",
       " [6, 21, 4, 8],\n",
       " [49, 10, 20, 50],\n",
       " [51, 52, 53, 54],\n",
       " [55, 56],\n",
       " [12, 2, 3, 13],\n",
       " [14, 2, 3, 5],\n",
       " [6, 4, 7, 57, 4, 7],\n",
       " [15, 2, 3, 5],\n",
       " [77, 78, 79],\n",
       " [80, 81],\n",
       " [82, 83],\n",
       " [84, 85, 86, 45],\n",
       " [40, 19, 20, 16],\n",
       " [6, 32, 46, 8, 33],\n",
       " [31, 47],\n",
       " [9, 9, 9],\n",
       " [34, 35, 36, 37],\n",
       " [48, 6, 21, 4, 8],\n",
       " [22, 10, 11, 23],\n",
       " [17, 7, 24, 25],\n",
       " [26, 4, 18, 27],\n",
       " [12, 2, 3, 13],\n",
       " [14, 2, 3, 5],\n",
       " [28, 29, 30],\n",
       " [15, 2, 3, 5],\n",
       " [6, 21, 4, 8],\n",
       " [49, 10, 20, 50],\n",
       " [51, 52, 53, 54],\n",
       " [55, 56],\n",
       " [12, 2, 3, 13],\n",
       " [14, 2, 3, 5],\n",
       " [6, 4, 7, 57, 4, 7],\n",
       " [15, 2, 3, 5],\n",
       " [1, 1, 1, 1, 38],\n",
       " [1, 1, 1, 1]]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 우선, 토큰화 (위 단계들 요약)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "#토크나이저를 선언하고\n",
    "tokenizer.fit_on_texts(text)\n",
    "#토크나이저에 텍스트를 학습시키고\n",
    "enc = tokenizer.texts_to_sequences(text)\n",
    "#텍스트를 인코딩한다.\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "81788312-90e4-4e89-9391-168d9888f6d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  1,  1,  1,  1, 38],\n",
       "       [ 0,  0,  0,  1,  1,  1,  1],\n",
       "       [ 0,  0,  0, 22, 10, 11, 23],\n",
       "       [ 0,  0,  0, 17,  7, 24, 25],\n",
       "       [ 0,  0,  0, 26,  4, 18, 27],\n",
       "       [ 0,  0,  0, 12,  2,  3, 13],\n",
       "       [ 0,  0,  0, 14,  2,  3,  5],\n",
       "       [ 0,  0,  0,  0, 28, 29, 30],\n",
       "       [ 0,  0,  0, 15,  2,  3,  5],\n",
       "       [ 0,  0,  0,  0,  0, 58, 59],\n",
       "       [ 0,  0,  0,  0,  0, 60, 61],\n",
       "       [ 0,  0,  0, 31, 62, 39, 63],\n",
       "       [ 0,  0,  0, 64, 65, 66, 67],\n",
       "       [ 0,  0,  0, 40, 19, 20, 16],\n",
       "       [ 0,  0,  0, 41,  7,  4, 42],\n",
       "       [ 0,  0,  0, 17,  6,  4, 42],\n",
       "       [ 0,  0,  0,  0, 68, 43, 69],\n",
       "       [41,  7, 70, 71, 72, 73, 44],\n",
       "       [ 0,  0,  0, 19, 11, 16, 44],\n",
       "       [ 0,  0,  0,  0, 19, 11, 16],\n",
       "       [ 0,  6, 32, 18, 16,  8, 33],\n",
       "       [ 0,  0,  0,  0, 39, 74, 75],\n",
       "       [ 0,  0,  0,  0, 45, 43, 76],\n",
       "       [ 0,  0,  0, 34, 35, 36, 37],\n",
       "       [ 0,  0,  6, 32, 46,  8, 33],\n",
       "       [ 0,  0,  0,  0,  0, 31, 47],\n",
       "       [ 0,  0,  0,  0,  9,  9,  9],\n",
       "       [ 0,  0,  0, 34, 35, 36, 37],\n",
       "       [ 0,  0, 48,  6, 21,  4,  8],\n",
       "       [ 0,  0,  0, 22, 10, 11, 23],\n",
       "       [ 0,  0,  0, 17,  7, 24, 25],\n",
       "       [ 0,  0,  0, 26,  4, 18, 27],\n",
       "       [ 0,  0,  0, 12,  2,  3, 13],\n",
       "       [ 0,  0,  0, 14,  2,  3,  5],\n",
       "       [ 0,  0,  0,  0, 28, 29, 30],\n",
       "       [ 0,  0,  0, 15,  2,  3,  5],\n",
       "       [ 0,  0,  0,  6, 21,  4,  8],\n",
       "       [ 0,  0,  0, 49, 10, 20, 50],\n",
       "       [ 0,  0,  0, 51, 52, 53, 54],\n",
       "       [ 0,  0,  0,  0,  0, 55, 56],\n",
       "       [ 0,  0,  0, 12,  2,  3, 13],\n",
       "       [ 0,  0,  0, 14,  2,  3,  5],\n",
       "       [ 0,  6,  4,  7, 57,  4,  7],\n",
       "       [ 0,  0,  0, 15,  2,  3,  5],\n",
       "       [ 0,  0,  0,  0, 77, 78, 79],\n",
       "       [ 0,  0,  0,  0,  0, 80, 81],\n",
       "       [ 0,  0,  0,  0,  0, 82, 83],\n",
       "       [ 0,  0,  0, 84, 85, 86, 45],\n",
       "       [ 0,  0,  0, 40, 19, 20, 16],\n",
       "       [ 0,  0,  6, 32, 46,  8, 33],\n",
       "       [ 0,  0,  0,  0,  0, 31, 47],\n",
       "       [ 0,  0,  0,  0,  9,  9,  9],\n",
       "       [ 0,  0,  0, 34, 35, 36, 37],\n",
       "       [ 0,  0, 48,  6, 21,  4,  8],\n",
       "       [ 0,  0,  0, 22, 10, 11, 23],\n",
       "       [ 0,  0,  0, 17,  7, 24, 25],\n",
       "       [ 0,  0,  0, 26,  4, 18, 27],\n",
       "       [ 0,  0,  0, 12,  2,  3, 13],\n",
       "       [ 0,  0,  0, 14,  2,  3,  5],\n",
       "       [ 0,  0,  0,  0, 28, 29, 30],\n",
       "       [ 0,  0,  0, 15,  2,  3,  5],\n",
       "       [ 0,  0,  0,  6, 21,  4,  8],\n",
       "       [ 0,  0,  0, 49, 10, 20, 50],\n",
       "       [ 0,  0,  0, 51, 52, 53, 54],\n",
       "       [ 0,  0,  0,  0,  0, 55, 56],\n",
       "       [ 0,  0,  0, 12,  2,  3, 13],\n",
       "       [ 0,  0,  0, 14,  2,  3,  5],\n",
       "       [ 0,  6,  4,  7, 57,  4,  7],\n",
       "       [ 0,  0,  0, 15,  2,  3,  5],\n",
       "       [ 0,  0,  1,  1,  1,  1, 38],\n",
       "       [ 0,  0,  0,  1,  1,  1,  1]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기서 보면, 0이 앞에 채워지고, 값이 있는 것들이 뒤로 빠진다.\n",
    "# -> 이유 : 초기의 neural network에서는 뒷쪽에 있는 것을 더 의미 있게 인식하기 때문 (10년 전의 나보다 어제의 내가 중요)\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded = pad_sequences(enc)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "f723f6a5-bbe1-4aeb-864e-7f6490704883",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  1,  1, 38,  0,  0],\n",
       "       [ 1,  1,  1,  1,  0,  0,  0],\n",
       "       [22, 10, 11, 23,  0,  0,  0],\n",
       "       [17,  7, 24, 25,  0,  0,  0],\n",
       "       [26,  4, 18, 27,  0,  0,  0],\n",
       "       [12,  2,  3, 13,  0,  0,  0],\n",
       "       [14,  2,  3,  5,  0,  0,  0],\n",
       "       [28, 29, 30,  0,  0,  0,  0],\n",
       "       [15,  2,  3,  5,  0,  0,  0],\n",
       "       [58, 59,  0,  0,  0,  0,  0],\n",
       "       [60, 61,  0,  0,  0,  0,  0],\n",
       "       [31, 62, 39, 63,  0,  0,  0],\n",
       "       [64, 65, 66, 67,  0,  0,  0],\n",
       "       [40, 19, 20, 16,  0,  0,  0],\n",
       "       [41,  7,  4, 42,  0,  0,  0],\n",
       "       [17,  6,  4, 42,  0,  0,  0],\n",
       "       [68, 43, 69,  0,  0,  0,  0],\n",
       "       [41,  7, 70, 71, 72, 73, 44],\n",
       "       [19, 11, 16, 44,  0,  0,  0],\n",
       "       [19, 11, 16,  0,  0,  0,  0],\n",
       "       [ 6, 32, 18, 16,  8, 33,  0],\n",
       "       [39, 74, 75,  0,  0,  0,  0],\n",
       "       [45, 43, 76,  0,  0,  0,  0],\n",
       "       [34, 35, 36, 37,  0,  0,  0],\n",
       "       [ 6, 32, 46,  8, 33,  0,  0],\n",
       "       [31, 47,  0,  0,  0,  0,  0],\n",
       "       [ 9,  9,  9,  0,  0,  0,  0],\n",
       "       [34, 35, 36, 37,  0,  0,  0],\n",
       "       [48,  6, 21,  4,  8,  0,  0],\n",
       "       [22, 10, 11, 23,  0,  0,  0],\n",
       "       [17,  7, 24, 25,  0,  0,  0],\n",
       "       [26,  4, 18, 27,  0,  0,  0],\n",
       "       [12,  2,  3, 13,  0,  0,  0],\n",
       "       [14,  2,  3,  5,  0,  0,  0],\n",
       "       [28, 29, 30,  0,  0,  0,  0],\n",
       "       [15,  2,  3,  5,  0,  0,  0],\n",
       "       [ 6, 21,  4,  8,  0,  0,  0],\n",
       "       [49, 10, 20, 50,  0,  0,  0],\n",
       "       [51, 52, 53, 54,  0,  0,  0],\n",
       "       [55, 56,  0,  0,  0,  0,  0],\n",
       "       [12,  2,  3, 13,  0,  0,  0],\n",
       "       [14,  2,  3,  5,  0,  0,  0],\n",
       "       [ 6,  4,  7, 57,  4,  7,  0],\n",
       "       [15,  2,  3,  5,  0,  0,  0],\n",
       "       [77, 78, 79,  0,  0,  0,  0],\n",
       "       [80, 81,  0,  0,  0,  0,  0],\n",
       "       [82, 83,  0,  0,  0,  0,  0],\n",
       "       [84, 85, 86, 45,  0,  0,  0],\n",
       "       [40, 19, 20, 16,  0,  0,  0],\n",
       "       [ 6, 32, 46,  8, 33,  0,  0],\n",
       "       [31, 47,  0,  0,  0,  0,  0],\n",
       "       [ 9,  9,  9,  0,  0,  0,  0],\n",
       "       [34, 35, 36, 37,  0,  0,  0],\n",
       "       [48,  6, 21,  4,  8,  0,  0],\n",
       "       [22, 10, 11, 23,  0,  0,  0],\n",
       "       [17,  7, 24, 25,  0,  0,  0],\n",
       "       [26,  4, 18, 27,  0,  0,  0],\n",
       "       [12,  2,  3, 13,  0,  0,  0],\n",
       "       [14,  2,  3,  5,  0,  0,  0],\n",
       "       [28, 29, 30,  0,  0,  0,  0],\n",
       "       [15,  2,  3,  5,  0,  0,  0],\n",
       "       [ 6, 21,  4,  8,  0,  0,  0],\n",
       "       [49, 10, 20, 50,  0,  0,  0],\n",
       "       [51, 52, 53, 54,  0,  0,  0],\n",
       "       [55, 56,  0,  0,  0,  0,  0],\n",
       "       [12,  2,  3, 13,  0,  0,  0],\n",
       "       [14,  2,  3,  5,  0,  0,  0],\n",
       "       [ 6,  4,  7, 57,  4,  7,  0],\n",
       "       [15,  2,  3,  5,  0,  0,  0],\n",
       "       [ 1,  1,  1,  1, 38,  0,  0],\n",
       "       [ 1,  1,  1,  1,  0,  0,  0]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다만, 0을 뒤로 채우는 방법 또한 있다.\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded = pad_sequences(enc, padding='post')\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc2f747-95ea-4c9d-a8a8-d5b35b69f333",
   "metadata": {},
   "source": [
    "## 사이킷런을 위한 패딩  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e90320ad-00ce-4a96-92e7-50655d3be045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "2996c0f0-7d60-4b7a-a283-7f7f0771a0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'자연어': 1,\n",
       " '나는': 2,\n",
       " '처리를': 3,\n",
       " '배우는': 4,\n",
       " '거겠지': 5,\n",
       " '처리는': 6,\n",
       " '어렵지': 7,\n",
       " '않기를': 8,\n",
       " '바라': 9}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"나는 자연어 처리를 배우는 거겠지 자연어 처리는 어렵지 않기를 바라\"  #텍스트 선언\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts([text])  #토크나이저로 텍스트 학습\n",
    "tok.word_index  #토크나이징 된 것에 인덱스 뽑아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "13945752-dd51-4bc4-8c0e-45afea5b39ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 7, 8, 9]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1단계 : 정수 인코딩\n",
    "sub_text = \"나는 어렵지 않기를 바라\"\n",
    "enc1 = tok.texts_to_sequences([sub_text])[0] #서브 텍스트에 포함된 단어들을 tok의 인덱스를 적용해 뽑아보기\n",
    "enc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "c3548c46-18bb-4fa9-8768-56b2612065a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2단계 : 원핫인코딩\n",
    "ohe1 = to_categorical(enc1)\n",
    "ohe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "41f40f78-dd40-467e-a5be-efd2b201fbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pad</th>\n",
       "      <th>자연어</th>\n",
       "      <th>나는</th>\n",
       "      <th>처리를</th>\n",
       "      <th>배우는</th>\n",
       "      <th>거겠지</th>\n",
       "      <th>처리는</th>\n",
       "      <th>어렵지</th>\n",
       "      <th>않기를</th>\n",
       "      <th>바라</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pad  자연어   나는  처리를  배우는  거겠지  처리는  어렵지  않기를   바라\n",
       "0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0\n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원핫 인코딩의 결과를 좀 더 명확하게 보자\n",
    "col = ['pad', '자연어', '나는', '처리를', '배우는', '거겠지', '처리는', '어렵지', '않기를', '바라']\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(ohe1, columns = col)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e1b322-896b-4217-8df0-dffd165c909f",
   "metadata": {},
   "source": [
    "## 참고 : 원핫 인코딩과 레이블 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c28a826d-5a5d-415a-87fc-a3da8ebc179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [\"빨강\", \"파랑\", \"파랑\", \"초록\", \"빨강\", \"검정\", \"하양\", \"파랑\"]\n",
    "alive = [\"alive\", \"dead\", \"dead\", \"alive\", \"alive\", \"dead\", \"dead\", \"dead\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "0653f82c-19de-49ad-96ff-313b7dbef415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블 인코딩 :  [1 3 3 2 1 0 4 3]\n",
      "생존 여부 :  [0 1 1 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# 레이블 인코딩\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "print(\"레이블 인코딩 : \", le.fit_transform(items))\n",
    "print(\"생존 여부 : \", le.fit_transform(alive))\n",
    "\n",
    "## 레이블 인코딩은 숫자가 크면 생존한다라고 오해할 수 있으며\n",
    "## 그게 아니더라도 \"어라 왜 검정은 숫자가 작은데 alive했지? 라는 오해로 학습이 잘 안될 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "81aa8cc1-2aa6-435c-ac3d-707b7a128177",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\sesac_jh\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:808: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 1.]])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원핫 인코딩 \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame([items, alive]).T\n",
    "ohe = OneHotEncoder(sparse = False)\n",
    "ohe.fit_transform(df1)\n",
    "\n",
    "## 원 핫 인코딩은 (1)연산에는 부담을 주고 (2)종합적으로 보기엔 혼란을 줄 수 있으나\n",
    "## 그래도, 학습에서의 오해를 주지 않으므로, 좋은 방법이라고 선생님은 판단한다.\n",
    "\n",
    "#케라스를 통한 원핫인코딩도 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a98055-34e0-4fb4-ac47-c2d9af56e4f4",
   "metadata": {},
   "source": [
    "* 원 핫 인코딩 : 하나만 hot (1) 이기 때문  \n",
    "* 반대로 하나를 뺀 모두가 1인 경우 원 콜드 인코딩이라고 한다. cold(0)  \n",
    "* 이는 디지털 회로 혹은 머신 러닝의 관점으로 1과 0을 바라봤기 때문에 이같이 이름이 부여된 듯 하다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3638c2b1-bed7-46ba-b634-6157e1e5e7a2",
   "metadata": {},
   "source": [
    "# ■■■ 여기서 분리 필요 ■■■"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45c4c3d-37b2-495f-839d-949b4e404913",
   "metadata": {},
   "source": [
    "# 언어 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1a80fd-9359-4356-807e-cba02825dfa4",
   "metadata": {},
   "source": [
    "* 예전 초기 자연어처리에서는 음성의 단어 인식이 화두였고 주류였다.  \n",
    "* 이를 위한 주파수 분석 -> 단어 매칭의 단계로 진행이 되는 기술이었다.\n",
    "\n",
    "이러한 접근방향 때문에 생긴 문제가 있는데\n",
    "\n",
    "ex. 주어진 문장 : \"아버지가 방에 들어가신다\"\n",
    "기계가 인식한 문장 : \"아버지와 빵에 돌아가신다\"\n",
    "\n",
    "바로 '문맥'의 의미를 잃어버리는 문제가 발생했다.\n",
    "\n",
    "* 이를 보완하기 위해 문맥의 중요성을 넣어줬는데.. (2000년대 초반)\n",
    "* 앞 단어와 어울릴 만한 뒷 단어에 대한 데이터 (표)를 만들어 운영을 했다.\n",
    "* 오타 교정, 음성 인식 교정, 추천 등의 기능을 가진. 언어 모델\n",
    "\n",
    "ex. 주어진 문장 : \"아버지가 방에 들어가신다\"\n",
    "(1) 인식한 문장 : 아버지가 방에 돌아가신다 - (기계적 90% 일치, 문맥상 20% 일치)\n",
    "(2) 인식한 문장 : 아버지가 방에 들어가신다 - (기계적 80% 일치, 문맥상 90% 일치)\n",
    "-> 2번 문장을 인식한 것으로 진행한다.\n",
    "\n",
    "ex. 자기야 잘 잤어? --> 라는 단어를 많이 입력했다면..\n",
    "(1) 자기 --> 라고 타이핑시 추천 : (야, 도, ...)\n",
    "(2) 자기야 --> 선택시 추천 : (잘, 우리, ...)\n",
    "(3) 자기야 잘 --> 선택시 추천 : (잤어?, 있어?, ...)\n",
    "라고 추천을 하는 시스템\n",
    "\n",
    "* 요즘에는 딥러닝으로 이러한 문맥 판단, 자연어 처리를 진행을 한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c468e0d-112a-4347-bb39-78c1dd2b8bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
