{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71620e98-f189-4bba-a106-933d1f7b9f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "# EarlyStopping : \n",
    "# Model Chechpoint : \n",
    "\n",
    "from preprocess import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59b5efb4-6135-4565-8cc2-11f474a00219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_' + string])\n",
    "    plt.legend([string], 'val_' + string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d838cff2-6fd0-4014-bdad-44aa5b2d1310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 저장\n",
    "\n",
    "DATA_IN_PATH = 'data_in/'\n",
    "DATA_OUT_PATH = 'data_out/'\n",
    "\n",
    "TRAIN_INPUTS = 'train_inputs.npy'\n",
    "TRAIN_OUTPUTS = 'train_outputs.npy'\n",
    "TRAIN_TARGETS = 'train_targets.npy'\n",
    "\n",
    "DATA_CONFIGS = 'data_config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "667382cb-f26e-4080-8946-19f806cdb7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd12894b-2f83-47f2-8fcf-439f3825babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_inputs = np.load(open(DATA_IN_PATH + TRAIN_INPUTS, 'rb'))\n",
    "index_outputs = np.load(open(DATA_IN_PATH + TRAIN_OUTPUTS, 'rb'))\n",
    "index_targets = np.load(open(DATA_IN_PATH + TRAIN_TARGETS, 'rb'))\n",
    "\n",
    "prepro_configs = json.load(open(DATA_IN_PATH + DATA_CONFIGS, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c10c53f5-9597-487e-918f-76e1dcea7cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11823 11823 11823\n"
     ]
    }
   ],
   "source": [
    "print(len(index_inputs), len(index_outputs), len(index_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9171be8-1207-43e8-9c4e-7537a6ed51ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'seq2seq_kor'\n",
    "BATCH_SIZE = 2\n",
    "MAX_SEQUENCE = 25\n",
    "EPOCH = 50\n",
    "UNITS = 1024\n",
    "EMBEDDING_DIM = 256\n",
    "VALIDATION_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0de3bce0-35df-452b-a3b0-69cb03a97c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['char2idx', 'idx2char', 'vocab_size', 'pad_symbol', 'std_symbol', 'end_symbol', 'unk_symbol'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepro_configs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2b1d53b-59a7-4bac-8405-9b70575cc487",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = prepro_configs['char2idx']\n",
    "idx2char = prepro_configs['idx2char']\n",
    "std_symbol = prepro_configs['std_symbol']\n",
    "end_symbol = prepro_configs['end_symbol']\n",
    "unk_symbol = prepro_configs['unk_symbol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6426870b-8cc5-40b2-bbdb-ca2e4c270488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, batch_sz):\n",
    "        super(Encoder, self).__init__()        # 상속 와 이렇게 쓰는거구나...\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True,\n",
    "                                      recurrent_initializer = 'glorot_uniform')\n",
    "    \n",
    "    \n",
    "    def call(self):\n",
    "        x = self.embedding(x)\n",
    "        self.gru(x, initial_state = hidden)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def initialize_hidden_state(self, ):\n",
    "        return tf.zeros(tf.shape(inp)[0], self.enc_units)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "723e5ad2-4075-45fd-bec1-8babe4e764a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, ):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, query, value):\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.W1(values) + self.W2(hidden_with_time_axis) # W1 : value에 관련된.. W2 : 쿼리에 관련된 것\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c251c93-b7b0-4761-87f3-df405c5110a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오... 층을 아래에서부터 쌓는 것도 좋네\n",
    "# 그러면 원하는 최종 결과물이 무엇인지를 보고, 그 결과가 나올 수 있게끔 필요한 것들과 양을 더 쉽게 정할 수 있을 듯\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, ):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dec_units = dec_units\n",
    "        self.batch_sz = batch_sz\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences = True, return_state = True,\n",
    "                                      recurrent_initializer = 'glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(self.vocab_size)\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        \n",
    "    def call(self, x, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        tf.concat([tf.expand_dims(context_vector, 1), x], axis = -1)\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1909648a-3f92-4b69-a43c-7e6605f5e546",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
